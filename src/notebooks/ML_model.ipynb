{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Changes the current path to find the source files\n",
    "current_dir = os.getcwd()\n",
    "while current_dir != os.path.abspath(\"../src\"):\n",
    "    os.chdir(\"..\")\n",
    "    current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))\n",
    "\n",
    "OUTPUT_DIR = \"../data/others/model_output\"\n",
    "\n",
    "folder_paths = [OUTPUT_DIR]\n",
    "\n",
    "# Create the output directories if they doesn't exist\n",
    "for folder_path in folder_paths:\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from plot import get_bounding_boxes, create_bboxes_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from layers import AMF_GD_YOLOv8\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 96.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476k/476k [00:00<00:00, 41.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/alexandre/Documents/tree-segmentation/src/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 47.1ms\n",
      "Speed: 4.1ms preprocess, 47.1ms inference, 119.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.models.yolo import YOLO\n",
    "\n",
    "# Load a pretrained YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the AMF GD YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/annotations_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m chm_image_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/CHM_cropped_0p24/122000_484000/122000_484000_1_1_filtered_chm.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m annotations_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/annotations_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m gt_bboxes, gt_classes \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m output_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_output_test.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, output_name)\n",
      "File \u001b[0;32m~/Documents/tree-segmentation/src/plot.py:15\u001b[0m, in \u001b[0;36mget_bounding_boxes\u001b[0;34m(bboxes_path, normalize)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bounding_boxes\u001b[39m(\n\u001b[1;32m     13\u001b[0m     bboxes_path: \u001b[38;5;28mstr\u001b[39m, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[List[\u001b[38;5;28mfloat\u001b[39m]], List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbboxes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Load the annotation data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         bboxes_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Get every bounding box\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/annotations_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.json'"
     ]
    }
   ],
   "source": [
    "class_names = {\n",
    "    0: \"Tree\",\n",
    "    1: \"Tree_unsure\",\n",
    "    2: \"Tree_disappeared\",\n",
    "    3: \"Tree_replaced\",\n",
    "    4: \"Tree_new\",\n",
    "}\n",
    "class_indices = {value: key for key, value in class_names.items()}\n",
    "\n",
    "model = AMF_GD_YOLOv8(3, 1, scale=\"s\", class_names=class_names)\n",
    "\n",
    "rgb_image_path = \"../data/images/cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.tif\"\n",
    "chm_image_path = (\n",
    "    \"../data/CHM_cropped_0p24/122000_484000/122000_484000_1_1_filtered_chm.tif\"\n",
    ")\n",
    "annotations_path = \"../data/annotations/cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.json\"\n",
    "\n",
    "gt_bboxes, gt_classes = get_bounding_boxes(annotations_path)\n",
    "\n",
    "output_name = \"Model_output_test.jpg\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "\n",
    "model.train()\n",
    "output = model(rgb_image_path, chm_image_path, image_save_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6728, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n",
      "{'loss_class': tensor(1.6705, grad_fn=<MulBackward0>), 'loss_bbox': tensor(19366.4082, grad_fn=<SqueezeBackward0>), 'loss_giou': tensor(2.9315, grad_fn=<SqueezeBackward0>), 'loss_class_aux': tensor(0.), 'loss_bbox_aux': tensor(0.), 'loss_giou_aux': tensor(0.)}\n"
     ]
    }
   ],
   "source": [
    "# from ultralytics.models.utils.loss import DETRLoss\n",
    "# from layers import xywh2xyxy\n",
    "\n",
    "# nc = 4\n",
    "# loss_func = DETRLoss(nc=nc)\n",
    "\n",
    "# # output[0][0][0, :, 0] = torch.Tensor([107.3209, 151.2296, 161.4145, 207.6499, 0.9837, 0.5472, 0.3150, 0.5183])\n",
    "\n",
    "# pred_bboxes = output[0][0].unsqueeze(0).permute((0, 1, 3, 2))[:, :, :, :4]\n",
    "# pred_scores = output[0][0].unsqueeze(0).permute((0, 1, 3, 2))[:, :, :, 4:]\n",
    "# batch = {\n",
    "#     \"cls\": torch.Tensor([class_indices[cls] for cls in classes]).to(dtype=torch.int64),\n",
    "#     \"bboxes\": xywh2xyxy(torch.Tensor(bboxes)),\n",
    "#     \"gt_groups\": [len(classes)],\n",
    "# }\n",
    "\n",
    "# prefect_pred_bboxes = batch[\"bboxes\"].unsqueeze(0).unsqueeze(0)\n",
    "# prefect_pred_scores = (\n",
    "#     torch.where(\n",
    "#         batch[\"cls\"].unsqueeze(-1).repeat(1, nc)\n",
    "#         == torch.arange(nc).unsqueeze(0).repeat(batch[\"cls\"].shape[0], 1),\n",
    "#         1.0,\n",
    "#         -1.0,\n",
    "#     )\n",
    "#     .unsqueeze(0)\n",
    "#     .unsqueeze(0)\n",
    "# )\n",
    "\n",
    "# loss = loss_func.forward(pred_bboxes, pred_scores, batch)\n",
    "# print(f\"{loss = }\")\n",
    "\n",
    "model.train()\n",
    "for i in range(100):\n",
    "    output = model(rgb_image_path, chm_image_path, image_save_path=output_path)\n",
    "    loss = model.compute_loss(output[0], gt_bboxes, gt_classes)\n",
    "    print(loss)\n",
    "    (loss[\"loss_class\"] + loss[\"loss_bbox\"]).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 100\n",
    "\n",
    "dataloader = ...\n",
    "model = AMF_GD_YOLOv8(3, 1, scale=\"n\", class_names=class_names)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: 1 / np.sqrt(i + 2), last_epoch=-1, verbose=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        rgb_image = ...\n",
    "        chm_image = ...\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(rgb_image, chm_image)\n",
    "        loss = model.compute_loss(output[0], gt_bboxes, gt_classes)\n",
    "        total_loss = loss[\"loss_class\"] + loss[\"loss_bbox\"]\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Loss class: {loss[\"loss_class\"]:.4f}, Loss bbox: {loss[\"loss_bbox\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use data augmentation with Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Dict, Any\n",
    "\n",
    "\n",
    "class Pad(A.PadIfNeeded):\n",
    "    def __init__(\n",
    "        self,\n",
    "        added_height: int | None = 64,\n",
    "        added_width: int | None = 64,\n",
    "        pad_height_divisor: int | None = None,\n",
    "        pad_width_divisor: int | None = None,\n",
    "        position: A.PadIfNeeded.PositionType | str = A.PadIfNeeded.PositionType.CENTER,\n",
    "        border_mode: int = cv2.BORDER_REFLECT_101,\n",
    "        value: float | Sequence[float] | None = None,\n",
    "        mask_value: float | Sequence[float] | None = None,\n",
    "        always_apply: bool = False,\n",
    "        p: float = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            0,\n",
    "            0,\n",
    "            pad_height_divisor,\n",
    "            pad_width_divisor,\n",
    "            position,\n",
    "            border_mode,\n",
    "            value,\n",
    "            mask_value,\n",
    "            always_apply,\n",
    "            p,\n",
    "        )\n",
    "        self.added_height = added_height\n",
    "        self.added_width = added_width\n",
    "\n",
    "    def update_params(self, params: Dict[str, Any], **kwargs: Any) -> Dict[str, Any]:\n",
    "        params = super().update_params(params, **kwargs)\n",
    "        rows = params[\"rows\"]\n",
    "        cols = params[\"cols\"]\n",
    "        self.min_height = rows + self.added_height\n",
    "        self.min_width = cols + self.added_width\n",
    "        return super().update_params(params, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2.006\n",
      "Time elapsed: 1.43\n"
     ]
    }
   ],
   "source": [
    "crop_size = 640\n",
    "distort_steps = 30\n",
    "distort_limit = 0.2\n",
    "\n",
    "bbox_params = A.BboxParams(\n",
    "    format=\"coco\", min_area=0, min_visibility=0.2, label_fields=[\"class_labels\"]\n",
    ")\n",
    "\n",
    "transform_spatial = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.GridDistortion(\n",
    "            num_steps=distort_steps,\n",
    "            distort_limit=(-distort_limit, distort_limit),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            normalized=True,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.Perspective(interpolation=cv2.INTER_LINEAR, p=0.25),\n",
    "    ],\n",
    "    bbox_params=bbox_params,\n",
    ")\n",
    "\n",
    "transform_pixel = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.Sharpen(p=0.25),\n",
    "        A.RingingOvershoot(p=0.5),\n",
    "        A.RandomGamma(p=1.0),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.FancyPCA(alpha=1.0, p=0.5),\n",
    "        A.Emboss(p=0.5),\n",
    "        A.Blur(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "        A.ChannelDropout(p=0.25),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.25),\n",
    "        # A.Normalize(p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# image_path = \"../data/images_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_2_3.tif\"\n",
    "# annotations_path = \"../data/annotations_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_2_3.json\"\n",
    "image_path = \"../data/images_full/2023_122000_484000_RGB_hrl.tif\"\n",
    "annotations_path = \"../data/annotations_full/3_all_annotations.json\"\n",
    "\n",
    "t = time()\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "bboxes, labels = get_bounding_boxes(annotations_path)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")\n",
    "\n",
    "number_tests = 10\n",
    "\n",
    "colors = {\n",
    "    \"Tree\": (104, 201, 45),\n",
    "    \"Tree_unsure\": (255, 215, 158),\n",
    "    \"Tree_disappeared\": (158, 174, 255),\n",
    "    \"Tree_replaced\": (255, 90, 82),\n",
    "    \"Tree_new\": (251, 106, 225),\n",
    "}\n",
    "\n",
    "t = time()\n",
    "\n",
    "for i in range(number_tests):\n",
    "    transformed_spatial = transform_spatial(\n",
    "        image=image, bboxes=bboxes, class_labels=labels\n",
    "    )\n",
    "    transformed_spatial_image = transformed_spatial[\"image\"]\n",
    "    transformed_bboxes = transformed_spatial[\"bboxes\"]\n",
    "    transformed_class_labels = transformed_spatial[\"class_labels\"]\n",
    "    transformed = transform_pixel(image=transformed_spatial_image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, transformed_image)\n",
    "\n",
    "    bboxes_image = create_bboxes_image(\n",
    "        transformed_image,\n",
    "        transformed_bboxes,\n",
    "        labels=transformed_class_labels,\n",
    "        colors_dict=colors,\n",
    "        scores=np.random.rand((len(bboxes))),\n",
    "    )\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}_bboxes.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, bboxes_image)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.012\n",
      "Time elapsed: 0.085\n"
     ]
    }
   ],
   "source": [
    "crop_size = 640\n",
    "distort_steps = 30\n",
    "distort_limit = 0.2\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "transform_2 = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.Sharpen(p=0.25),\n",
    "        A.RingingOvershoot(p=0.5),\n",
    "        A.RandomGamma(p=1.0),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.FancyPCA(alpha=1.0, p=0.5),\n",
    "        A.Emboss(p=0.5),\n",
    "        A.Blur(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "        A.ChannelDropout(p=0.25),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.25),\n",
    "        A.Normalize(p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "image_path = \"../data/images_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_2_3.tif\"\n",
    "# image_path = \"../data/flat-design-abstract-outline-background_23-2150616456.jpg\"\n",
    "\n",
    "t = time()\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")\n",
    "\n",
    "number_tests = 1\n",
    "\n",
    "t = time()\n",
    "\n",
    "for i in range(number_tests):\n",
    "    r = random.random()\n",
    "    random.seed(r)\n",
    "    transformed_image = transform(image=image)[\"image\"]\n",
    "    random.seed(r)\n",
    "    transformed_image_2 = transform_2(image=image)[\"image\"]\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, transformed_image)\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}_2.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, transformed_image_2)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
