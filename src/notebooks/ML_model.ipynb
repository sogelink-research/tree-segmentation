{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Changes the current path to find the source files\n",
    "current_dir = os.getcwd()\n",
    "while current_dir != os.path.abspath(\"../src\"):\n",
    "    os.chdir(\"..\")\n",
    "    current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))\n",
    "\n",
    "OUTPUT_DIR = \"../data/others/model_output\"\n",
    "\n",
    "folder_paths = [OUTPUT_DIR]\n",
    "\n",
    "# Create the output directories if they doesn't exist\n",
    "for folder_path in folder_paths:\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from plot import get_bounding_boxes, create_bboxes_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from layers import AMF_GD_YOLOv8\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import Folders\n",
    "from data_processing import ImageData\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Sequence, Dict, Any, Callable, List, Tuple\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_file_base_name\n",
    "from skimage import io\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
    ")\n",
    "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo import YOLO\n",
    "from ultralytics.models.yolo.detect.predict import DetectionPredictor\n",
    "\n",
    "# Load a pretrained YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "# results = model.train(data=\"coco8.yaml\", epochs=3, verbose=False)\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model.predict(\"https://ultralytics.com/images/bus.jpg\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the AMF GD YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from ultralytics.engine.trainer import BaseTrainer\n",
    "from ultralytics.models.yolo.detect.train import DetectionTrainer\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "\n",
    "\n",
    "# class CustomTrainer(BaseTrainer):\n",
    "#     def __init__(\n",
    "#         self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None\n",
    "#     ):\n",
    "#         super().__init__(cfg, overrides, _callbacks)\n",
    "#         self.model = self.args.model\n",
    "\n",
    "#     def get_model(self, cfg=None, weights=None, verbose=True):\n",
    "#         # model = DetectionModel(cfg, nc=self.data[\"nc\"], verbose=verbose and RANK == -1)\n",
    "#         # if weights:\n",
    "#         #     model.load(weights)\n",
    "#         # return model\n",
    "#         return self.model\n",
    "\n",
    "\n",
    "class_names = {\n",
    "    0: \"Tree\",\n",
    "    1: \"Tree_unsure\",\n",
    "    2: \"Tree_disappeared\",\n",
    "    3: \"Tree_replaced\",\n",
    "    4: \"Tree_new\",\n",
    "}\n",
    "\n",
    "class_indices = {value: key for key, value in class_names.items()}\n",
    "\n",
    "# model = AMF_GD_YOLOv8(3, 1, device=device, scale=\"s\", class_names=class_names)\n",
    "\n",
    "# overrides = {\"model\": model, \"data\": \"coco8.yaml\"}\n",
    "\n",
    "\n",
    "class TestModel(AMF_GD_YOLOv8):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_input_left: int,\n",
    "        c_input_right: int,\n",
    "        class_names: Dict[int, str],\n",
    "        device: torch.device,\n",
    "        scale: str = \"n\",\n",
    "        r: int = 16,\n",
    "        gd_config_file: str | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            c_input_left, c_input_right, class_names, device, scale, r, gd_config_file\n",
    "        )\n",
    "\n",
    "        class Args:\n",
    "            def __init__(self) -> None:\n",
    "                self.box = 1.0\n",
    "                self.cls = 1.0\n",
    "                self.dfl = 1.0\n",
    "\n",
    "        self.args = Args()\n",
    "        self.model = nn.ModuleList([self.amfnet, self.gd, self.detect])\n",
    "\n",
    "\n",
    "# class v8DetectionLossTest(v8DetectionLoss):\n",
    "#     def __init__(self, model):\n",
    "#         \"\"\"Initializes v8DetectionLoss with the model, defining model-related properties and BCE loss function.\"\"\"\n",
    "#         device = next(model.parameters()).device  # get model device\n",
    "#         h = model.args  # hyperparameters\n",
    "\n",
    "#         m = model.model[-1]  # Detect() module\n",
    "#         self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "#         self.hyp = h\n",
    "#         self.stride = m.stride  # model strides\n",
    "#         self.nc = m.nc  # number of classes\n",
    "#         self.no = m.nc + m.reg_max * 4\n",
    "#         self.reg_max = m.reg_max\n",
    "#         self.device = device\n",
    "\n",
    "#         self.use_dfl = m.reg_max > 1\n",
    "\n",
    "#         self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "#         self.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)\n",
    "#         self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "model_test = TestModel(3, 1, device=device, scale=\"s\", class_names=class_names)\n",
    "\n",
    "criterion = v8DetectionLoss(model_test)\n",
    "\n",
    "# results = model.train(data=\"coco8.yaml\", trainer=CustomTrainer, epochs=3, overrides=overrides)\n",
    "\n",
    "# rgb_image_path = \"../data/images/cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.tif\"\n",
    "# chm_image_path = (\n",
    "#     \"../data/CHM_cropped_0p24/122000_484000/122000_484000_1_1_filtered_chm.tif\"\n",
    "# )\n",
    "# annotations_path = \"../data/annotations/cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_1.json\"\n",
    "\n",
    "# gt_bboxes, gt_classes = get_bounding_boxes(annotations_path)\n",
    "\n",
    "# output_name = \"Model_output_test.jpg\"\n",
    "# output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "\n",
    "# model.train()\n",
    "# output = model(rgb_image_path, chm_image_path, image_save_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics.models.utils.loss import DETRLoss\n",
    "# from layers import xywh2xyxy\n",
    "\n",
    "# nc = 4\n",
    "# loss_func = DETRLoss(nc=nc)\n",
    "\n",
    "# # output[0][0][0, :, 0] = torch.Tensor([107.3209, 151.2296, 161.4145, 207.6499, 0.9837, 0.5472, 0.3150, 0.5183])\n",
    "\n",
    "# pred_bboxes = output[0][0].unsqueeze(0).permute((0, 1, 3, 2))[:, :, :, :4]\n",
    "# pred_scores = output[0][0].unsqueeze(0).permute((0, 1, 3, 2))[:, :, :, 4:]\n",
    "# batch = {\n",
    "#     \"cls\": torch.Tensor([class_indices[cls] for cls in classes]).to(dtype=torch.int64),\n",
    "#     \"bboxes\": xywh2xyxy(torch.Tensor(bboxes)),\n",
    "#     \"gt_groups\": [len(classes)],\n",
    "# }\n",
    "\n",
    "# prefect_pred_bboxes = batch[\"bboxes\"].unsqueeze(0).unsqueeze(0)\n",
    "# prefect_pred_scores = (\n",
    "#     torch.where(\n",
    "#         batch[\"cls\"].unsqueeze(-1).repeat(1, nc)\n",
    "#         == torch.arange(nc).unsqueeze(0).repeat(batch[\"cls\"].shape[0], 1),\n",
    "#         1.0,\n",
    "#         -1.0,\n",
    "#     )\n",
    "#     .unsqueeze(0)\n",
    "#     .unsqueeze(0)\n",
    "# )\n",
    "\n",
    "# loss = loss_func.forward(pred_bboxes, pred_scores, batch)\n",
    "# print(f\"{loss = }\")\n",
    "\n",
    "model.train()\n",
    "for i in range(100):\n",
    "    output = model(rgb_image_path, chm_image_path, image_save_path=output_path)\n",
    "    loss = model.compute_loss(output[0], gt_bboxes, gt_classes)\n",
    "    print(loss)\n",
    "    (loss[\"loss_class\"] + loss[\"loss_bbox\"]).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "dataloader = ...\n",
    "model = AMF_GD_YOLOv8(3, 1, scale=\"n\", class_names=class_names)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: 1 / np.sqrt(i + 2), last_epoch=-1, verbose=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        rgb_image = ...\n",
    "        chm_image = ...\n",
    "\n",
    "        output = model(rgb_image, chm_image)\n",
    "        loss = model.compute_loss(output[0], gt_bboxes, gt_classes)\n",
    "        # total_loss = loss[\"loss_class\"] + loss[\"loss_bbox\"]\n",
    "        total_loss = torch.sum(torch.stack(list(loss.values())), dim=0).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Loss class: {loss[\"loss_class\"]:.4f}, Loss bbox: {loss[\"loss_bbox\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use data augmentation with Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(A.PadIfNeeded):\n",
    "    def __init__(\n",
    "        self,\n",
    "        added_height: int | None = 64,\n",
    "        added_width: int | None = 64,\n",
    "        pad_height_divisor: int | None = None,\n",
    "        pad_width_divisor: int | None = None,\n",
    "        position: A.PadIfNeeded.PositionType | str = A.PadIfNeeded.PositionType.CENTER,\n",
    "        border_mode: int = cv2.BORDER_REFLECT_101,\n",
    "        value: float | Sequence[float] | None = None,\n",
    "        mask_value: float | Sequence[float] | None = None,\n",
    "        always_apply: bool = False,\n",
    "        p: float = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            0,\n",
    "            0,\n",
    "            pad_height_divisor,\n",
    "            pad_width_divisor,\n",
    "            position,\n",
    "            border_mode,\n",
    "            value,\n",
    "            mask_value,\n",
    "            always_apply,\n",
    "            p,\n",
    "        )\n",
    "        self.added_height = added_height\n",
    "        self.added_width = added_width\n",
    "\n",
    "    def update_params(self, params: Dict[str, Any], **kwargs: Any) -> Dict[str, Any]:\n",
    "        params = super().update_params(params, **kwargs)\n",
    "        rows = params[\"rows\"]\n",
    "        cols = params[\"cols\"]\n",
    "        self.min_height = rows + self.added_height\n",
    "        self.min_width = cols + self.added_width\n",
    "        return super().update_params(params, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 640\n",
    "distort_steps = 30\n",
    "distort_limit = 0.2\n",
    "\n",
    "bbox_params = A.BboxParams(\n",
    "    format=\"pascal_voc\", min_area=0, min_visibility=0.2, label_fields=[\"class_labels\"]\n",
    ")\n",
    "\n",
    "transform_spatial = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.GridDistortion(\n",
    "            num_steps=distort_steps,\n",
    "            distort_limit=(-distort_limit, distort_limit),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            normalized=True,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.Perspective(interpolation=cv2.INTER_LINEAR, p=0.25),\n",
    "    ],\n",
    "    bbox_params=bbox_params,\n",
    ")\n",
    "\n",
    "transform_pixel = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.Sharpen(p=0.25),\n",
    "        A.RingingOvershoot(p=0.5),\n",
    "        A.RandomGamma(p=1.0),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.FancyPCA(alpha=1.0, p=0.5),\n",
    "        A.Emboss(p=0.5),\n",
    "        A.Blur(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "        A.ChannelDropout(p=0.25),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.25),\n",
    "        # A.Normalize(p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "image_path = \"../data/images/cropped/2023_122000_484000_RGB_hrl/0_0_640_640.tif\"\n",
    "annotations_path = (\n",
    "    \"../data/annotations/cropped/2023_122000_484000_RGB_hrl/0_0_640_640.json\"\n",
    ")\n",
    "# image_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
    "# annotations_path = \"../data/annotations/full/10\"\n",
    "\n",
    "t = time()\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "bboxes, labels = get_bounding_boxes(annotations_path)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")\n",
    "\n",
    "number_tests = 10\n",
    "\n",
    "colors = {\n",
    "    \"Tree\": (104, 201, 45),\n",
    "    \"Tree_unsure\": (255, 215, 158),\n",
    "    \"Tree_disappeared\": (158, 174, 255),\n",
    "    \"Tree_replaced\": (255, 90, 82),\n",
    "    \"Tree_new\": (251, 106, 225),\n",
    "}\n",
    "\n",
    "t = time()\n",
    "\n",
    "for i in range(number_tests):\n",
    "    transformed_spatial = transform_spatial(\n",
    "        image=image, bboxes=[bbox.as_list() for bbox in bboxes], class_labels=labels\n",
    "    )\n",
    "    transformed_spatial_image = transformed_spatial[\"image\"]\n",
    "    transformed_bboxes = transformed_spatial[\"bboxes\"]\n",
    "    transformed_class_labels = transformed_spatial[\"class_labels\"]\n",
    "    transformed = transform_pixel(image=transformed_spatial_image)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, transformed_image)\n",
    "\n",
    "    bboxes_image = create_bboxes_image(\n",
    "        transformed_image,\n",
    "        transformed_bboxes,\n",
    "        labels=transformed_class_labels,\n",
    "        colors_dict=colors,\n",
    "        scores=np.random.rand((len(bboxes))),\n",
    "    )\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}_bboxes.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    cv2.imwrite(output_path, bboxes_image)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
    "resolution = 0.08\n",
    "\n",
    "image_data = ImageData(full_image_path)\n",
    "\n",
    "annotations_folder_path = os.path.join(\n",
    "    Folders.CROPPED_ANNOTS.value, image_data.base_name\n",
    ")\n",
    "\n",
    "rgb_folder_path = os.path.join(Folders.CROPPED_IMAGES.value, image_data.base_name)\n",
    "chm_folder_path = os.path.join(\n",
    "    Folders.CHM.value,\n",
    "    f\"{round(resolution*100)}cm\",\n",
    "    \"unfiltered\",\n",
    "    \"cropped\",\n",
    "    image_data.coord_name,\n",
    ")\n",
    "\n",
    "means_rgb = torch.zeros(3)\n",
    "means_chm = 0\n",
    "count = 0\n",
    "\n",
    "for file_name in tqdm(os.listdir(annotations_folder_path)):\n",
    "    annotations_file_path = os.path.join(annotations_folder_path, file_name)\n",
    "    base_name = get_file_base_name(annotations_file_path)\n",
    "    rgb_path = os.path.join(rgb_folder_path, f\"{base_name}.tif\")\n",
    "    image_rgb = io.imread(rgb_path)\n",
    "    chm_path = os.path.join(chm_folder_path, f\"{base_name}.tif\")\n",
    "    image_chm = io.imread(chm_path)\n",
    "\n",
    "    means_rgb += np.std(image_rgb, axis=(0, 1))\n",
    "    image_chm = np.where(image_chm == -9999, 0, image_chm)\n",
    "    means_chm += np.std(image_chm)\n",
    "    count += 1\n",
    "\n",
    "mean_rgb = means_rgb / count\n",
    "mean_chm = means_chm / count\n",
    "\n",
    "print(f\"{mean_rgb = }\")\n",
    "print(f\"{mean_chm = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torch.utils.data.dataloader import _collate_fn_t, _worker_init_fn_t\n",
    "\n",
    "\n",
    "def normalize_rgb(image_rgb: torch.Tensor) -> torch.Tensor:\n",
    "    ## On the whole image\n",
    "    # mean_rgb = torch.tensor([78.152, 88.417, 86.365]).view(-1, 1, 1)\n",
    "    # std_rgb = torch.tensor([45.819, 42.492, 38.960]).view(-1, 1, 1)\n",
    "    ## On the labeled parts\n",
    "    mean_rgb = torch.tensor([77.692, 89.142, 85.816]).view(-1, 1, 1)\n",
    "    std_rgb = torch.tensor([34.3328, 32.1673, 27.6653]).view(-1, 1, 1)\n",
    "    return (image_rgb - mean_rgb) / std_rgb\n",
    "\n",
    "\n",
    "def normalize_chm(image_chm: torch.Tensor) -> torch.Tensor:\n",
    "    image_chm = torch.where(image_chm == -9999, 0, image_chm)\n",
    "    ## On the whole image\n",
    "    # mean_chm = 2.4113\n",
    "    # std_chm = 5.5642\n",
    "    ## On the labeled parts\n",
    "    mean_chm = 3.0424\n",
    "    std_chm = 4.5557\n",
    "    return (image_chm - mean_chm) / std_chm\n",
    "\n",
    "\n",
    "# def xyxy2xywh(x):\n",
    "#     \"\"\"\n",
    "#     Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n",
    "#     top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "#     Args:\n",
    "#         x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "\n",
    "#     Returns:\n",
    "#         y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n",
    "#     \"\"\"\n",
    "#     if isinstance(x, list):\n",
    "#         x = torch.tensor(x)\n",
    "#     assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "#     y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "#     y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "#     y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "#     y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "#     y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "#     return y\n",
    "\n",
    "\n",
    "class TreeDataset(Dataset):\n",
    "    \"\"\"Tree dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_folder_path: str,  # TODO: Change to take a list of files instead (to separate train and validation)\n",
    "        rgb_folder_path: str,\n",
    "        chm_folder_path: str,\n",
    "        labels_to_index: Dict[str, int],\n",
    "        labels_to_color: Dict[str, Tuple[int, int, int]],\n",
    "        transform_spatial: Callable | None = None,\n",
    "        transform_pixel: Callable | None = None,\n",
    "    ) -> None:\n",
    "        self.annotations_list: List[str] = []\n",
    "        self.bboxes: Dict[str, List[List[float]]] = {}\n",
    "        self.labels: Dict[str, List[int]] = {}\n",
    "        for file_name in os.listdir(annotations_folder_path):\n",
    "            annotations_file_path = os.path.join(annotations_folder_path, file_name)\n",
    "            base_name = get_file_base_name(annotations_file_path)\n",
    "            self.annotations_list.append(base_name)\n",
    "            bboxes, labels = get_bounding_boxes(annotations_file_path)\n",
    "            self.bboxes[base_name] = [bbox.as_list() for bbox in bboxes]\n",
    "            self.labels[base_name] = [labels_to_index[label] for label in labels]\n",
    "\n",
    "        self.rgb_folder_path = rgb_folder_path\n",
    "        self.chm_folder_path = chm_folder_path\n",
    "        self.labels_to_index = labels_to_index\n",
    "        self.labels_to_str = {value: key for key, value in self.labels_to_index.items()}\n",
    "        self.labels_to_color = labels_to_color\n",
    "        self.transform_spatial = transform_spatial\n",
    "        self.transform_pixel = transform_pixel\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # return len(self.landmarks_frame)\n",
    "        return len(self.annotations_list)\n",
    "\n",
    "    def get_unnormalized(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "\n",
    "        # Read the images\n",
    "        base_name = self.annotations_list[idx]\n",
    "        rgb_path = os.path.join(self.rgb_folder_path, f\"{base_name}.tif\")\n",
    "        image_rgb = io.imread(rgb_path)\n",
    "        chm_path = os.path.join(self.chm_folder_path, f\"{base_name}.tif\")\n",
    "        image_chm = io.imread(chm_path)\n",
    "\n",
    "        # Get bboxes and labels\n",
    "        bboxes = self.bboxes[base_name]\n",
    "        labels = self.labels[base_name]\n",
    "\n",
    "        # Apply the spatial transform to the two images, bboxes and labels\n",
    "        if self.transform_spatial is not None:\n",
    "            transformed_spatial = transform_spatial(\n",
    "                image=image_rgb,\n",
    "                image_chm=image_chm,\n",
    "                bboxes=bboxes,\n",
    "                class_labels=labels,\n",
    "            )\n",
    "            image_rgb = transformed_spatial[\"image\"]\n",
    "            image_chm = transformed_spatial[\"image_chm\"]\n",
    "            bboxes = transformed_spatial[\"bboxes\"]\n",
    "            labels = transformed_spatial[\"class_labels\"]\n",
    "\n",
    "        # Apply the pixel transform the to RGB image\n",
    "        if transform_pixel is not None:\n",
    "            transformed = transform_pixel(image=image_rgb)\n",
    "            image_rgb = transformed[\"image\"]\n",
    "\n",
    "        to_tensor = ToTensorV2()\n",
    "\n",
    "        sample = {\n",
    "            \"image_rgb\": to_tensor(image=image_rgb)[\"image\"],\n",
    "            \"image_chm\": to_tensor(image=image_chm)[\"image\"],\n",
    "            \"bboxes\": torch.tensor(bboxes),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"image_index\": idx,\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.get_unnormalized(idx)\n",
    "        sample[\"image_rgb\"] = normalize_rgb(sample[\"image_rgb\"])\n",
    "        sample[\"image_chm\"] = normalize_chm(sample[\"image_chm\"])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def get_rgb_image(self, idx) -> np.ndarray:\n",
    "        base_name = self.annotations_list[idx]\n",
    "        rgb_path = os.path.join(self.rgb_folder_path, f\"{base_name}.tif\")\n",
    "        image_rgb = io.imread(rgb_path)\n",
    "        return image_rgb\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Initialize lists to hold the extracted components\n",
    "    rgb_images = []\n",
    "    chm_images = []\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    indices = []\n",
    "    image_indices = []\n",
    "\n",
    "    # Iterate through the batch\n",
    "    for i, item in enumerate(batch):\n",
    "        # Extract the components from the dictionary\n",
    "        rgb_image = item[\"image_rgb\"]\n",
    "        chm_image = item[\"image_chm\"]\n",
    "        bbox = item[\"bboxes\"]\n",
    "        label = item[\"labels\"]\n",
    "        image_index = item[\"image_index\"]\n",
    "\n",
    "        # Append the extracted components to the lists\n",
    "        rgb_images.append(rgb_image)\n",
    "        chm_images.append(chm_image)\n",
    "        bboxes.append(bbox)\n",
    "        labels.append(label)\n",
    "        indices.extend([i] * bbox.shape[0])\n",
    "        image_indices.append(image_index)\n",
    "\n",
    "    # Convert the lists to tensors and stack them\n",
    "    rgb_images = torch.stack(rgb_images, dim=0)\n",
    "    chm_images = torch.stack(chm_images, dim=0)\n",
    "    bboxes = torch.cat(bboxes).to(torch.float32)\n",
    "    labels = torch.cat(labels)\n",
    "    indices = torch.tensor(indices)\n",
    "    image_indices = torch.tensor(image_indices)\n",
    "\n",
    "    batch = {\n",
    "        \"image_rgb\": rgb_images,\n",
    "        \"image_chm\": chm_images,\n",
    "        \"bboxes\": bboxes,\n",
    "        \"labels\": labels,\n",
    "        \"indices\": indices,\n",
    "        \"image_indices\": image_indices,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "class TreeDataLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: TreeDataset,\n",
    "        batch_size: int | None = 1,\n",
    "        shuffle: bool | None = None,\n",
    "        sampler: Sampler | Iterable | None = None,\n",
    "        batch_sampler: Sampler[List] | Iterable[List] | None = None,\n",
    "        num_workers: int = 0,\n",
    "        collate_fn: Callable[[List], Any] | None = None,\n",
    "        pin_memory: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        timeout: float = 0,\n",
    "        worker_init_fn: Callable[[int], None] | None = None,\n",
    "        multiprocessing_context=None,\n",
    "        generator=None,\n",
    "        *,\n",
    "        prefetch_factor: int | None = None,\n",
    "        persistent_workers: bool = False,\n",
    "        pin_memory_device: str = \"\",\n",
    "    ):\n",
    "        self.dataset: TreeDataset = dataset\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            sampler,\n",
    "            batch_sampler,\n",
    "            num_workers,\n",
    "            collate_fn,\n",
    "            pin_memory,\n",
    "            drop_last,\n",
    "            timeout,\n",
    "            worker_init_fn,\n",
    "            multiprocessing_context,\n",
    "            generator,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "            persistent_workers=persistent_workers,\n",
    "            pin_memory_device=pin_memory_device,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0.0, \"count\": 0, \"avg\": 0.0})\n",
    "\n",
    "    def update(self, metric_name: str, val: float, count: int = 1):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += count\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                f\"{metric_name}: {metric[\"avg\"]:.{self.float_precision}f}\"\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "def perfect_preds(\n",
    "    gt_bboxes: torch.Tensor,\n",
    "    gt_classes: torch.Tensor,\n",
    "    gt_indices: torch.Tensor,\n",
    "    batch_size: int,\n",
    "):\n",
    "    extracted_bboxes = [[]] * batch_size\n",
    "    extracted_classes = [[]] * batch_size\n",
    "    for bbox_idx, image_idx in enumerate(gt_indices):\n",
    "        slice_bboxes = gt_bboxes[bbox_idx]\n",
    "        extracted_bboxes[image_idx].append(slice_bboxes)\n",
    "        slice_classes = gt_classes[bbox_idx].long()\n",
    "        extracted_classes[image_idx].append(slice_classes)\n",
    "    scores = [\n",
    "        20 * nn.functional.one_hot(torch.tensor(cls), num_classes=5) - 0.5\n",
    "        for cls in extracted_classes\n",
    "    ]\n",
    "    prefect_preds = [\n",
    "        torch.cat((torch.tensor(bboxes), classes), dim=1).permute((1, 0)).unsqueeze(0)\n",
    "        for bboxes, classes in zip(extracted_bboxes, scores)\n",
    "    ]\n",
    "    perfect_preds = torch.cat(\n",
    "        [\n",
    "            torch.cat(\n",
    "                (\n",
    "                    pred,\n",
    "                    torch.zeros(\n",
    "                        (pred.shape[0], pred.shape[1], 8400 - pred.shape[2])\n",
    "                    ).to(pred.device),\n",
    "                ),\n",
    "                dim=2,\n",
    "            )\n",
    "            for pred in prefect_preds\n",
    "        ]\n",
    "    )\n",
    "    # print(f\"{perfect_preds.shape = }\")\n",
    "    return perfect_preds\n",
    "\n",
    "\n",
    "def print_current_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        current_memory_usage_bytes = torch.cuda.memory_allocated()\n",
    "        current_memory_usage_megabytes = current_memory_usage_bytes / (1024 * 1024)\n",
    "        print(f\"Current GPU memory usage: {current_memory_usage_megabytes:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_loader: TreeDataLoader,\n",
    "    model: AMF_GD_YOLOv8,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    device: torch.device,\n",
    "    accumulation_steps: int,\n",
    "    running_accumulation_step: int,\n",
    ") -> int:\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    for i, data in enumerate(stream, start=running_accumulation_step):\n",
    "        image_rgb: torch.Tensor = data[\"image_rgb\"]\n",
    "        image_chm: torch.Tensor = data[\"image_chm\"]\n",
    "        gt_bboxes: torch.Tensor = data[\"bboxes\"]\n",
    "        gt_classes: torch.Tensor = data[\"labels\"]\n",
    "        gt_indices: torch.Tensor = data[\"indices\"]\n",
    "\n",
    "        image_rgb = image_rgb.to(device, non_blocking=True)\n",
    "        image_chm = image_chm.to(device, non_blocking=True)\n",
    "        gt_bboxes = gt_bboxes.to(device, non_blocking=True)\n",
    "        gt_classes = gt_classes.to(device, non_blocking=True)\n",
    "        gt_indices = gt_indices.to(device, non_blocking=True)\n",
    "\n",
    "        output = model(image_rgb, image_chm)\n",
    "        print(f\"@@@@@ : {gt_bboxes.mean() = }\")\n",
    "        total_loss = model.compute_loss(output, gt_bboxes, gt_classes, gt_indices)[0]\n",
    "\n",
    "        batch_size = image_rgb.shape[0]\n",
    "        metric_monitor.update(\"Loss\", total_loss.item(), batch_size)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # total_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        stream.set_description(f\"Epoch: {epoch}. Train.      {metric_monitor}\")\n",
    "\n",
    "    return running_accumulation_step\n",
    "\n",
    "\n",
    "def validate(\n",
    "    val_loader: TreeDataLoader, model: AMF_GD_YOLOv8, epoch: int, device: torch.device\n",
    "):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            image_rgb: torch.Tensor = data[\"image_rgb\"]\n",
    "            image_chm: torch.Tensor = data[\"image_chm\"]\n",
    "            gt_bboxes: torch.Tensor = data[\"bboxes\"]\n",
    "            gt_classes: torch.Tensor = data[\"labels\"]\n",
    "            gt_indices: torch.Tensor = data[\"indices\"]\n",
    "\n",
    "            image_rgb = image_rgb.to(device, non_blocking=True)\n",
    "            image_chm = image_chm.to(device, non_blocking=True)\n",
    "            gt_bboxes = gt_bboxes.to(device, non_blocking=True)\n",
    "            gt_classes = gt_classes.to(device, non_blocking=True)\n",
    "            gt_indices = gt_indices.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(image_rgb, image_chm)\n",
    "            print(f\"@@@@@ : {gt_bboxes.mean() = }\")\n",
    "            total_loss = model.compute_loss(output, gt_bboxes, gt_classes, gt_indices)[\n",
    "                0\n",
    "            ]\n",
    "            print(f\"#####################{total_loss = }\")\n",
    "\n",
    "            batch_size = image_rgb.shape[0]\n",
    "            metric_monitor.update(\"Loss\", total_loss.item(), batch_size)\n",
    "\n",
    "            stream.set_description(f\"Epoch: {epoch}. Validation. {metric_monitor}\")\n",
    "\n",
    "\n",
    "def test_save_output_image(\n",
    "    model: AMF_GD_YOLOv8,\n",
    "    test_loader: TreeDataLoader,\n",
    "    epoch: int,\n",
    "    device: torch.device,\n",
    "    number_images: int = 1,\n",
    "):\n",
    "    saved_images = 0\n",
    "    number_images = min(len(test_loader), number_images)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, start=1):\n",
    "            image_rgb: torch.Tensor = data[\"image_rgb\"]\n",
    "            image_chm: torch.Tensor = data[\"image_chm\"]\n",
    "            image_rgb = image_rgb.to(device, non_blocking=True)\n",
    "            image_chm = image_chm.to(device, non_blocking=True)\n",
    "            results = model.predict(image_rgb, image_chm)[2]\n",
    "\n",
    "            initial_rgb = test_loader.dataset.get_rgb_image(data[\"image_indices\"])\n",
    "            colors_dict = test_loader.dataset.labels_to_color\n",
    "            if results.boxes is not None:\n",
    "                bboxes = results.boxes.xyxy.tolist()\n",
    "                labels = [results.names[cls.item()] for cls in results.boxes.cls]\n",
    "                scores = results.boxes.conf.tolist()\n",
    "            else:\n",
    "                bboxes = []\n",
    "                labels = []\n",
    "                scores = []\n",
    "\n",
    "            bboxes_image = create_bboxes_image(\n",
    "                image=initial_rgb,\n",
    "                bboxes=bboxes,\n",
    "                labels=labels,\n",
    "                colors_dict=colors_dict,\n",
    "                scores=scores,\n",
    "                color_mode=\"bgr\",\n",
    "            )\n",
    "\n",
    "            output_name = f\"Validation_bboxes_{epoch}_{saved_images}_{len(bboxes)}.png\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "            cv2.imwrite(output_path, bboxes_image)\n",
    "\n",
    "            saved_images += 1\n",
    "            if saved_images >= number_images:\n",
    "                break\n",
    "\n",
    "\n",
    "def train_and_validate(\n",
    "    model: AMF_GD_YOLOv8,\n",
    "    train_dataset: TreeDataset,\n",
    "    val_dataset: TreeDataset,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    accumulate: int,\n",
    "    device: torch.device,\n",
    ") -> nn.Module:\n",
    "    train_loader = TreeDataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    val_loader = TreeDataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    test_loader = TreeDataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda i: 1 / np.sqrt(i + 2), last_epoch=-1, verbose=True\n",
    "    )\n",
    "\n",
    "    accumulation_steps = round(accumulate / batch_size)\n",
    "    running_accumulation_step = 0\n",
    "\n",
    "    test_save_output_image(\n",
    "        model=model, test_loader=test_loader, epoch=0, device=device, number_images=2\n",
    "    )\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_accumulation_step = train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            device,\n",
    "            accumulation_steps,\n",
    "            running_accumulation_step,\n",
    "        )\n",
    "        print_current_memory()\n",
    "        validate(val_loader, model, epoch, device)\n",
    "        print_current_memory()\n",
    "        if epoch % 1 == 0:\n",
    "            test_save_output_image(\n",
    "                model=model,\n",
    "                test_loader=test_loader,\n",
    "                epoch=epoch,\n",
    "                device=device,\n",
    "                number_images=2,\n",
    "            )\n",
    "            print_current_memory()\n",
    "        scheduler.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_test_test(\n",
    "    model: AMF_GD_YOLOv8,\n",
    "    val_dataset: TreeDataset,\n",
    "    epoch: int,\n",
    "    device: torch.device,\n",
    "    number_images: int = 1,\n",
    "):\n",
    "    saved_images = 0\n",
    "    number_images = min(len(val_dataset), number_images)\n",
    "    model.eval()\n",
    "    while saved_images < number_images:\n",
    "        sample = val_dataset[saved_images]\n",
    "        image_rgb: torch.Tensor = sample[\"image_rgb\"].unsqueeze(0)\n",
    "        image_chm: torch.Tensor = sample[\"image_chm\"].unsqueeze(0)\n",
    "        image_rgb = image_rgb.to(device, non_blocking=True)\n",
    "        image_chm = image_chm.to(device, non_blocking=True)\n",
    "        output = model.forward(image_rgb, image_chm)\n",
    "        print(f\"{output[0].shape}\")\n",
    "        print(f\"{output[1].shape}\")\n",
    "        print(f\"{output[2].shape}\")\n",
    "\n",
    "        gt_bboxes: torch.Tensor = sample[\"bboxes\"]\n",
    "        gt_classes: torch.Tensor = sample[\"labels\"]\n",
    "        gt_indices: torch.Tensor = torch.zeros(gt_bboxes.shape[0])\n",
    "        batch = {\"cls\": gt_classes, \"bboxes\": gt_bboxes, \"batch_idx\": gt_indices}\n",
    "\n",
    "        criterion = v8DetectionLoss(model_test)\n",
    "        criterion(output, batch)\n",
    "\n",
    "        saved_images += 1\n",
    "\n",
    "\n",
    "def tteeeeeeeest(\n",
    "    val_loader: DataLoader,\n",
    "    model: AMF_GD_YOLOv8,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.train()\n",
    "    stream = tqdm(val_loader)\n",
    "    # with torch.no_grad():\n",
    "    for i, data in enumerate(stream, start=1):\n",
    "        image_rgb: torch.Tensor = data[\"image_rgb\"]\n",
    "        image_chm: torch.Tensor = data[\"image_chm\"]\n",
    "        gt_bboxes: torch.Tensor = data[\"bboxes\"]\n",
    "        gt_classes: torch.Tensor = data[\"labels\"]\n",
    "        gt_indices: torch.Tensor = data[\"indices\"]\n",
    "\n",
    "        image_rgb = image_rgb.to(device, non_blocking=True)\n",
    "        image_chm = image_chm.to(device, non_blocking=True)\n",
    "        gt_bboxes = gt_bboxes.to(device, non_blocking=True)\n",
    "        gt_classes = gt_classes.to(device, non_blocking=True)\n",
    "        gt_indices = gt_indices.to(device, non_blocking=True)\n",
    "\n",
    "        output = model.forward(image_rgb, image_chm)\n",
    "        # print(f\"{output[0].shape}\")\n",
    "        # print(f\"{output[1].shape}\")\n",
    "        # print(f\"{output[2].shape}\")\n",
    "\n",
    "        # # # Create the list for indices\n",
    "        # # starts = gt_indices[:, 0].long().tolist()\n",
    "        # # ends = gt_indices[:, 1].long().tolist()\n",
    "        # # ranges = [end - start for start, end in zip(starts, ends)]\n",
    "        # # indices = torch.cat(\n",
    "        # #     [torch.full((count,), i) for i, count in enumerate(ranges)]\n",
    "        # # ).to(gt_classes.device)\n",
    "\n",
    "        # print(f\"{gt_classes.shape = }\")\n",
    "        # print(f\"{gt_bboxes.shape = }\")\n",
    "        # print(f\"{gt_indices.shape = }\")\n",
    "        # batch = {\"cls\": gt_classes, \"bboxes\": gt_bboxes, \"batch_idx\": gt_indices}\n",
    "\n",
    "        # criterion = v8DetectionLoss(model_test)\n",
    "        # total_loss = criterion(output, batch)[0]\n",
    "\n",
    "        total_loss = model.compute_loss(output, gt_bboxes, gt_classes, gt_indices)[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_current_memory()\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_current_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class_names = {\n",
    "    0: \"Tree\",\n",
    "    1: \"Tree_unsure\",\n",
    "    2: \"Tree_disappeared\",\n",
    "    3: \"Tree_replaced\",\n",
    "    4: \"Tree_new\",\n",
    "}\n",
    "\n",
    "class_colors = {\n",
    "    \"Tree\": (104, 201, 45),\n",
    "    \"Tree_unsure\": (255, 215, 158),\n",
    "    \"Tree_disappeared\": (158, 174, 255),\n",
    "    \"Tree_replaced\": (255, 90, 82),\n",
    "    \"Tree_new\": (251, 106, 225),\n",
    "}\n",
    "\n",
    "class_indices = {value: key for key, value in class_names.items()}\n",
    "\n",
    "model = AMF_GD_YOLOv8(3, 1, device=device, scale=\"n\", class_names=class_names).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# Transformations for data augmentation\n",
    "\n",
    "crop_size = 640\n",
    "distort_steps = 30\n",
    "distort_limit = 0.2\n",
    "\n",
    "transform_spatial = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.GridDistortion(\n",
    "            num_steps=distort_steps,\n",
    "            distort_limit=(-distort_limit, distort_limit),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            normalized=True,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=1.0),\n",
    "        A.Perspective(interpolation=cv2.INTER_LINEAR, p=0.25),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        min_area=0,\n",
    "        min_visibility=0.2,\n",
    "        label_fields=[\"class_labels\"],\n",
    "    ),\n",
    "    additional_targets={\"image_chm\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_pixel = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.Sharpen(p=0.25),\n",
    "        A.RingingOvershoot(p=0.5),\n",
    "        A.RandomGamma(p=1.0),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.FancyPCA(alpha=1.0, p=0.5),\n",
    "        A.Emboss(p=0.5),\n",
    "        A.Blur(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "        A.ChannelDropout(p=0.25),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.25),\n",
    "        # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Paths to the data\n",
    "\n",
    "full_image_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
    "resolution = 0.08\n",
    "\n",
    "image_data = ImageData(full_image_path)\n",
    "\n",
    "annotations_folder_path = os.path.join(\n",
    "    Folders.CROPPED_ANNOTS.value, image_data.base_name\n",
    ")\n",
    "rgb_folder_path = os.path.join(Folders.CROPPED_IMAGES.value, image_data.base_name)\n",
    "chm_folder_path = os.path.join(\n",
    "    Folders.CHM.value,\n",
    "    f\"{round(resolution*100)}cm\",\n",
    "    \"unfiltered\",\n",
    "    \"cropped\",\n",
    "    image_data.coord_name,\n",
    ")\n",
    "\n",
    "train_dataset = TreeDataset(\n",
    "    annotations_folder_path=annotations_folder_path,\n",
    "    rgb_folder_path=rgb_folder_path,\n",
    "    chm_folder_path=chm_folder_path,\n",
    "    labels_to_index=class_indices,\n",
    "    labels_to_color=class_colors,\n",
    "    transform_spatial=transform_spatial,\n",
    "    transform_pixel=transform_pixel,\n",
    "    # transform_spatial=None,\n",
    "    # transform_pixel=None,\n",
    ")\n",
    "\n",
    "val_dataset = TreeDataset(\n",
    "    annotations_folder_path=annotations_folder_path,\n",
    "    rgb_folder_path=rgb_folder_path,\n",
    "    chm_folder_path=chm_folder_path,\n",
    "    labels_to_index=class_indices,\n",
    "    labels_to_color=class_colors,\n",
    "    transform_spatial=None,\n",
    "    transform_pixel=None,\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "lr = 1e-2\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 2\n",
    "accumulate = 12\n",
    "\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    accumulate=accumulate,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# test_test_test(model_test, val_dataset, 0, device, 1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# test_save_output_image(\n",
    "#     model=model,\n",
    "#     val_dataset=val_dataset,\n",
    "#     epoch=0,\n",
    "#     device=device,\n",
    "#     number_images=2,\n",
    "# )\n",
    "# tteeeeeeeest(\n",
    "#     val_loader=DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=True,\n",
    "#         collate_fn=custom_collate_fn,\n",
    "#     ),\n",
    "#     model=model,\n",
    "#     optimizer=optimizer,\n",
    "#     epoch=0,\n",
    "#     device=device,\n",
    "# )\n",
    "# test_save_output_image(\n",
    "#     model=model,\n",
    "#     val_dataset=val_dataset,\n",
    "#     epoch=1,\n",
    "#     device=device,\n",
    "#     number_images=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_device_of_layers(model):\n",
    "#     for name, module in model.named_modules():\n",
    "#         try:\n",
    "#             device = next(iter(module.parameters())).device\n",
    "#             # if device != torch.device(\"cuda:0\"):\n",
    "#             #     print(f\"{name}: {device}\")\n",
    "#             print(f\"{name}: {device}\")\n",
    "#         except StopIteration:\n",
    "#             pass  # Skip modules without parameters\n",
    "\n",
    "\n",
    "# print_device_of_layers(model)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    try:\n",
    "        print(f\"{name}: {next(iter(module.parameters()))[0]}\")\n",
    "    except StopIteration:\n",
    "        pass  # Skip modules without parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
    "chm_path = \"../data/chm/8cm/filtered/full/122000_484000.tif\"\n",
    "# rgb_path = \"../data/images/cropped/2023_122000_484000_RGB_hrl/0_0_640_640.tif\"\n",
    "# chm_path = \"../data/chm/8cm/filtered/cropped/122000_484000/0_0_640_640.tif\"\n",
    "\n",
    "\n",
    "image_rgb = io.imread(rgb_path)\n",
    "image_chm = io.imread(chm_path)\n",
    "\n",
    "# RGB normalization\n",
    "\n",
    "# rgb_mean = np.zeros(3, dtype=np.float32)\n",
    "# rgb_std = np.zeros(3, dtype=np.float32)\n",
    "# for i in range(3):\n",
    "#     rgb_mean[i] = np.mean(image_rgb[:,:,i])\n",
    "#     rgb_std[i] = np.std(image_rgb[:,:,i])\n",
    "# print(f\"{rgb_mean = }\")\n",
    "# print(f\"{rgb_std = }\")\n",
    "\n",
    "image_rgb = normalize_rgb(image_rgb)\n",
    "\n",
    "# CHM normalization\n",
    "\n",
    "# chm_mean = np.mean(image_chm)\n",
    "# chm_std = np.std(image_chm)\n",
    "# print(f\"{chm_mean = }\")\n",
    "# print(f\"{chm_std = }\")\n",
    "\n",
    "image_chm = normalize_chm(image_chm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 640\n",
    "distort_steps = 30\n",
    "distort_limit = 0.2\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "transform_2 = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(width=crop_size, height=crop_size, p=1.0),\n",
    "        A.Sharpen(p=0.25),\n",
    "        A.RingingOvershoot(p=0.5),\n",
    "        A.RandomGamma(p=1.0),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        # A.FancyPCA(alpha=1.0, p=0.5),\n",
    "        A.Emboss(p=0.5),\n",
    "        A.Blur(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=1.0),\n",
    "        A.ChannelDropout(p=0.25),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.25),\n",
    "        # A.Normalize(p=1.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "image_path = \"../data/images/cropped/2023_122000_484000_RGB_hrl/0_0_640_640.tif\"\n",
    "# image_path = \"../data/flat-design-abstract-outline-background_23-2150616456.jpg\"\n",
    "\n",
    "t = time()\n",
    "\n",
    "image = io.imread(image_path)\n",
    "# image = (image - rgb_mean) / rgb_std\n",
    "# image = image.astype(np.float32)\n",
    "print(f\"{image.dtype = }\")\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")\n",
    "\n",
    "number_tests = 1\n",
    "\n",
    "t = time()\n",
    "\n",
    "for i in range(number_tests):\n",
    "    r = random.random()\n",
    "    random.seed(r)\n",
    "    transformed_image = transform(image=image)[\"image\"]\n",
    "    random.seed(r)\n",
    "    transformed_image_2 = transform_2(image=image)[\"image\"]\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    io.imsave(output_path, transformed_image)\n",
    "\n",
    "    output_name = f\"Augmentation_test_{i}_2.tif\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "    io.imsave(output_path, transformed_image_2)\n",
    "\n",
    "print(f\"Time elapsed: {round(time() - t, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
