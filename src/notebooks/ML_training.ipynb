{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Changes the current path to find the source files\n",
                "current_dir = os.getcwd()\n",
                "while current_dir != os.path.abspath(\"../src\"):\n",
                "    os.chdir(\"..\")\n",
                "    current_dir = os.getcwd()\n",
                "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils import create_all_folders, Folders\n",
                "\n",
                "create_all_folders()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from layers import AMF_GD_YOLOv8\n",
                "import torch\n",
                "from data_processing import ImageData\n",
                "from training_parameters import (\n",
                "    class_names,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    transform_pixel_rgb,\n",
                "    transform_spatial,\n",
                "    proba_drop_rgb,\n",
                "    proba_drop_chm,\n",
                "    labels_transformation_drop_chm,\n",
                "    labels_transformation_drop_rgb,\n",
                ")\n",
                "from training import (\n",
                "    train_and_validate,\n",
                "    create_and_save_splitted_datasets,\n",
                "    load_tree_datasets_from_split,\n",
                ")\n",
                "import multiprocessing as mp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "garbage_collection_threshold:0.6,max_split_size_mb:512\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
                "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find and split the data\n",
                "\n",
                "full_image_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
                "resolution = 0.08\n",
                "\n",
                "image_data = ImageData(full_image_path)\n",
                "\n",
                "annotations_folder_path = os.path.join(Folders.CROPPED_ANNOTS.value, image_data.base_name)\n",
                "rgb_folder_path = os.path.join(Folders.CROPPED_IMAGES.value, image_data.base_name)\n",
                "chm_folder_path = os.path.join(\n",
                "    Folders.CHM.value,\n",
                "    f\"{round(resolution*100)}cm\",\n",
                "    \"unfiltered\",\n",
                "    \"cropped\",\n",
                "    image_data.coord_name,\n",
                ")\n",
                "\n",
                "sets_ratios = [3, 1, 1]\n",
                "sets_names = [\"training\", \"validation\", \"test\"]\n",
                "data_split_file_path = \"../data/others/data_split.json\"\n",
                "dismissed_classes = []\n",
                "\n",
                "create_and_save_splitted_datasets(\n",
                "    rgb_folder_path,\n",
                "    chm_folder_path,\n",
                "    annotations_folder_path,\n",
                "    sets_ratios,\n",
                "    sets_names,\n",
                "    data_split_file_path,\n",
                "    random_seed=0,\n",
                ")\n",
                "\n",
                "datasets = load_tree_datasets_from_split(\n",
                "    data_split_file_path,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    proba_drop_rgb=proba_drop_rgb,\n",
                "    labels_transformation_drop_rgb=labels_transformation_drop_rgb,\n",
                "    proba_drop_chm=proba_drop_chm,\n",
                "    labels_transformation_drop_chm=labels_transformation_drop_chm,\n",
                "    dismissed_classes=dismissed_classes,\n",
                "    transform_spatial_training=transform_spatial,\n",
                "    transform_pixel_training=transform_pixel_rgb,\n",
                ")\n",
                "\n",
                "# Training parameters\n",
                "\n",
                "lr = 1e-2\n",
                "epochs = 800\n",
                "\n",
                "batch_size = 6\n",
                "num_workers = mp.cpu_count()\n",
                "accumulate = 12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a6a1a8b8c5404f9fbde40a9dc2cd9356",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3ebee0298e604bb586b6325d30a227eb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/29 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e524e03e20994f0f9d19b3f72de1b61c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Epoch:   0%|          | 0/800 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/alexandre/miniforge3/envs/tree-segment/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1716579264291/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
                        "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
                        "/home/alexandre/miniforge3/envs/tree-segment/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1716579264291/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
                        "  return F.conv2d(input, weight, bias, self.stride,\n"
                    ]
                }
            ],
            "source": [
                "index = 0\n",
                "model_name = f\"trained_model_{epochs}ep_{index}\"\n",
                "while os.path.exists(f\"../models/amf_gd_yolov8/{model_name}.pt\"):\n",
                "    index += 1\n",
                "    model_name = f\"trained_model_{epochs}ep_{index}\"\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    3, 1, device=device, scale=\"n\", class_names=class_names, name=model_name\n",
                ").to(device)\n",
                "\n",
                "final_model = train_and_validate(\n",
                "    model=model,\n",
                "    datasets=datasets,\n",
                "    lr=lr,\n",
                "    epochs=epochs,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    accumulate=accumulate,\n",
                "    device=device,\n",
                "    save_outputs=True,\n",
                ")\n",
                "state_dict = final_model.state_dict()\n",
                "torch.save(state_dict, f\"../models/amf_gd_yolov8/{model_name}.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Finish with: 9.32326626777649 second, num_workers=2\n",
                        "Finish with: 6.183119773864746 second, num_workers=4\n",
                        "Finish with: 5.941167593002319 second, num_workers=6\n",
                        "Finish with: 5.7705371379852295 second, num_workers=8\n"
                    ]
                }
            ],
            "source": [
                "# from time import time\n",
                "# import multiprocessing as mp\n",
                "# from training import TreeDataLoader\n",
                "\n",
                "# batch_size = 8\n",
                "\n",
                "# for num_workers in range(2, mp.cpu_count() + 2, 2):\n",
                "#     train_loader = TreeDataLoader(\n",
                "#         datasets[\"training\"],\n",
                "#         batch_size=batch_size,\n",
                "#         shuffle=True,\n",
                "#         num_workers=num_workers,\n",
                "#         pin_memory=True,\n",
                "#     )\n",
                "#     start = time()\n",
                "#     for epoch in range(1, 3):\n",
                "#         for i, data in enumerate(train_loader, 0):\n",
                "#             pass\n",
                "#     end = time()\n",
                "#     print(f\"Finish with: {end - start} second, num_workers={num_workers}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9ade539be896442ebf8439360eb4f063",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/29 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "58d8cce59e4a42828764b222c1422c11",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/29 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "90f66e8a69f640d787f5e200229b7583",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/29 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "752226e44ec74ff9b1c8447cf50febeb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/29 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from training import test_save_output_image, initialize_dataloaders\n",
                "\n",
                "index = 0\n",
                "model_name = f\"trained_model_{epochs}ep_{index}\"\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    3, 1, device=device, scale=\"n\", class_names=class_names, name=model_name\n",
                ").to(device)\n",
                "\n",
                "state_dict = torch.load(f\"../models/amf_gd_yolov8/{model_name}.pt\")\n",
                "model.load_state_dict(state_dict)\n",
                "\n",
                "_, _, test_loader = initialize_dataloaders(\n",
                "    datasets=datasets, batch_size=batch_size, num_workers=num_workers\n",
                ")\n",
                "\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=False,\n",
                "    no_chm=False,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_all.geojson\",\n",
                ")\n",
                "\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=True,\n",
                "    no_chm=False,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_rgb.geojson\",\n",
                ")\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=False,\n",
                "    no_chm=True,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_chm.geojson\",\n",
                ")\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=True,\n",
                "    no_chm=True,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_rgb_no_chm.geojson\",\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tree-segment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}