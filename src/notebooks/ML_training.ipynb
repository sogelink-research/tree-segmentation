{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Changes the current path to find the source files\n",
    "current_dir = os.getcwd()\n",
    "while current_dir != os.path.abspath(\"../src\"):\n",
    "    os.chdir(\"..\")\n",
    "    current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_all_folders, Folders\n",
    "\n",
    "create_all_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import AMF_GD_YOLOv8\n",
    "import torch\n",
    "from utils import Folders\n",
    "from data_processing import ImageData\n",
    "from training_parameters import (\n",
    "    class_names,\n",
    "    class_indices,\n",
    "    class_colors,\n",
    "    transform_pixel,\n",
    "    transform_spatial,\n",
    ")\n",
    "from training import (\n",
    "    TreeDataset,\n",
    "    train_and_validate,\n",
    "    create_and_save_splitted_datasets,\n",
    "    load_tree_datasets_from_split,\n",
    ")\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garbage_collection_threshold:0.6,max_split_size_mb:512\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
    ")\n",
    "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fa5aedf8394ecfba3c06a2fcc2ed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defb79b7a2a14ac788c9a1b984675280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Epoch:   0%|          | 0/100 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/miniforge3/envs/tree-segment/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1716579264291/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "model = AMF_GD_YOLOv8(3, 1, device=device, scale=\"n\", class_names=class_names).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# Find and split the data\n",
    "\n",
    "full_image_path = \"../data/images/full/2023_122000_484000_RGB_hrl.tif\"\n",
    "resolution = 0.08\n",
    "\n",
    "image_data = ImageData(full_image_path)\n",
    "\n",
    "annotations_folder_path = os.path.join(\n",
    "    Folders.CROPPED_ANNOTS.value, image_data.base_name\n",
    ")\n",
    "rgb_folder_path = os.path.join(Folders.CROPPED_IMAGES.value, image_data.base_name)\n",
    "chm_folder_path = os.path.join(\n",
    "    Folders.CHM.value,\n",
    "    f\"{round(resolution*100)}cm\",\n",
    "    \"unfiltered\",\n",
    "    \"cropped\",\n",
    "    image_data.coord_name,\n",
    ")\n",
    "\n",
    "sets_ratios = [3, 1]\n",
    "sets_names = [\"training\", \"validation\"]\n",
    "data_split_file_path = \"../data/others/data_split.json\"\n",
    "dismissed_classes = [\"Tree_unsure\"]\n",
    "\n",
    "create_and_save_splitted_datasets(\n",
    "    rgb_folder_path,\n",
    "    chm_folder_path,\n",
    "    annotations_folder_path,\n",
    "    sets_ratios,\n",
    "    sets_names,\n",
    "    data_split_file_path,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "datasets = load_tree_datasets_from_split(\n",
    "    data_split_file_path,\n",
    "    class_indices,\n",
    "    class_colors,\n",
    "    dismissed_classes=dismissed_classes,\n",
    "    transform_spatial_training=transform_spatial,\n",
    "    transform_pixel_training=transform_pixel,\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "lr = 1e-2\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 12\n",
    "num_workers = mp.cpu_count()\n",
    "accumulate = 12\n",
    "\n",
    "final_model = train_and_validate(\n",
    "    model=model,\n",
    "    datasets=datasets,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    accumulate=accumulate,\n",
    "    device=device,\n",
    ")\n",
    "state_dict = final_model.state_dict()\n",
    "index = 1\n",
    "while os.path.exists(f\"../models/amf_gd_yolov8/trained_model_{index}.pt\"):\n",
    "    index += 1\n",
    "torch.save(state_dict, f\"../models/amf_gd_yolov8/trained_model_{index}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with: 9.793368101119995 second, num_workers=2\n",
      "Finish with: 7.073188543319702 second, num_workers=4\n",
      "Finish with: 6.6157708168029785 second, num_workers=6\n",
      "Finish with: 6.237012624740601 second, num_workers=8\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import multiprocessing as mp\n",
    "from training import TreeDataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for num_workers in range(2, mp.cpu_count() + 2, 2):\n",
    "    train_loader = TreeDataLoader(\n",
    "        datasets[\"training\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    start = time()\n",
    "    for epoch in range(1, 3):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            pass\n",
    "    end = time()\n",
    "    print(f\"Finish with: {end - start} second, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training import test_save_output_image, TreeDataLoader\n",
    "\n",
    "# model = AMF_GD_YOLOv8(3, 1, device=device, scale=\"n\", class_names=class_names).to(\n",
    "#     device\n",
    "# )\n",
    "\n",
    "# state_dict = torch.load(\"../models/amf_gd_yolov8/trained_model_1.pt\")\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# test_loader = TreeDataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=1,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# test_save_output_image(model, test_loader, -1, device, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
