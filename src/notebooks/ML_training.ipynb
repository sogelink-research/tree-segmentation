{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Changes the current path to find the source files\n",
                "current_dir = os.getcwd()\n",
                "while current_dir != os.path.abspath(\"../src\"):\n",
                "    os.chdir(\"..\")\n",
                "    current_dir = os.getcwd()\n",
                "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils import create_all_folders, Folders\n",
                "\n",
                "create_all_folders()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from layers import AMF_GD_YOLOv8\n",
                "import torch\n",
                "from preprocessing.data import ImageData\n",
                "from training_parameters import (\n",
                "    class_names,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    transform_pixel_rgb,\n",
                "    transform_pixel_chm,\n",
                "    transform_spatial,\n",
                "    proba_drop_rgb,\n",
                "    proba_drop_chm,\n",
                "    labels_transformation_drop_chm,\n",
                "    labels_transformation_drop_rgb,\n",
                ")\n",
                "from training import (\n",
                "    train_and_validate,\n",
                "    create_and_save_splitted_datasets,\n",
                "    load_tree_datasets_from_split,\n",
                "    compute_mean_and_std,\n",
                ")\n",
                "import multiprocessing as mp\n",
                "from geojson_conversions import open_geojson_feature_collection\n",
                "from preprocessing.rgb_cir import download_rgb_image_from_polygon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "garbage_collection_threshold:0.6,max_split_size_mb:512\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
                "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Download skipped: there is already a file at '/home/alexandre/Documents/tree-segmentation/data/images/rgb/full/2023_122000_484000_RGB_hrl.tif'.\n",
                        "mean_rgb = tensor([ 81.7854,  91.3560,  89.6526, 112.8617, 117.4514, 121.8722])\n",
                        "mean_chm = tensor([-2844.7755], dtype=torch.float64)\n"
                    ]
                }
            ],
            "source": [
                "# Find and split the data\n",
                "\n",
                "annotations_file_name = \"122000_484000.geojson\"\n",
                "\n",
                "annotations_path = os.path.join(Folders.FULL_ANNOTS.value, annotations_file_name)\n",
                "annotations = open_geojson_feature_collection(annotations_path)\n",
                "full_image_path_tif = download_rgb_image_from_polygon(annotations[\"bbox\"])[\n",
                "    0\n",
                "]  # TODO: Change this (create a new function)\n",
                "\n",
                "resolution = 0.08\n",
                "\n",
                "image_data = ImageData(full_image_path_tif)\n",
                "\n",
                "annotations_folder_path = os.path.join(Folders.CROPPED_ANNOTS.value, image_data.base_name)\n",
                "rgb_cir_folder_path = os.path.join(Folders.IMAGES.value, \"merged\", \"cropped\", image_data.base_name)\n",
                "chm_folder_path = os.path.join(\n",
                "    Folders.CHM.value,\n",
                "    f\"{round(resolution*100)}cm\",\n",
                "    \"filtered\",\n",
                "    \"merged\",\n",
                "    \"cropped\",\n",
                "    image_data.coord_name,\n",
                ")\n",
                "\n",
                "sets_ratios = [3, 1, 1]\n",
                "sets_names = [\"training\", \"validation\", \"test\"]\n",
                "data_split_file_path = \"../data/others/data_split.json\"\n",
                "dismissed_classes = []\n",
                "\n",
                "create_and_save_splitted_datasets(\n",
                "    rgb_cir_folder_path,\n",
                "    chm_folder_path,\n",
                "    annotations_folder_path,\n",
                "    sets_ratios,\n",
                "    sets_names,\n",
                "    data_split_file_path,\n",
                "    random_seed=0,\n",
                ")\n",
                "\n",
                "mean_rgb, std_rgb = compute_mean_and_std(rgb_cir_folder_path, per_channel=True)\n",
                "mean_chm, std_chm = compute_mean_and_std(chm_folder_path, per_channel=False)\n",
                "print(f\"{mean_rgb = }\")\n",
                "print(f\"{mean_chm = }\")\n",
                "\n",
                "datasets = load_tree_datasets_from_split(\n",
                "    data_split_file_path,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    mean_rgb=mean_rgb,\n",
                "    std_rgb=std_rgb,\n",
                "    mean_chm=mean_chm,\n",
                "    std_chm=std_chm,\n",
                "    proba_drop_rgb=proba_drop_rgb,\n",
                "    labels_transformation_drop_rgb=labels_transformation_drop_rgb,\n",
                "    proba_drop_chm=proba_drop_chm,\n",
                "    labels_transformation_drop_chm=labels_transformation_drop_chm,\n",
                "    dismissed_classes=dismissed_classes,\n",
                "    transform_spatial_training=transform_spatial,\n",
                "    transform_pixel_rgb_training=transform_pixel_rgb,\n",
                "    transform_pixel_chm_training=transform_pixel_chm,\n",
                ")\n",
                "\n",
                "# Training parameters\n",
                "\n",
                "lr = 1e-2\n",
                "epochs = 500\n",
                "\n",
                "batch_size = 6\n",
                "num_workers = mp.cpu_count()\n",
                "accumulate = 12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "datasets['training'].rgb_channels = 6\n",
                        "datasets['training'].chm_channels = 9\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3cf9fd4e73e14948a183a9598bb47c3e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "63045947616a4cd2a144ffa339c38a66",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Exporting output:   0%|          | 0/48 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6c48c8495fef408490aca6875f39c9df",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Epoch:   0%|          | 0/2 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/alexandre/miniforge3/envs/tree-segment/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1716579264291/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
                        "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
                    ]
                }
            ],
            "source": [
                "from typing import Tuple\n",
                "\n",
                "\n",
                "def get_model_name_and_path(postfix: str) -> Tuple[str, str]:\n",
                "\n",
                "    def _get_name_and_path(index: int) -> Tuple[str, str]:\n",
                "        model_name = f\"trained_model_{epochs}ep_{index}_{postfix}\"\n",
                "        model_path = os.path.join(Folders.MODELS_AMF_GD_YOLOV8.value, f\"{model_name}.pt\")\n",
                "        return model_name, model_path\n",
                "\n",
                "    index = 0\n",
                "    model_name, model_path = _get_name_and_path(index)\n",
                "    while os.path.exists(model_path):\n",
                "        index += 1\n",
                "        model_name, model_path = _get_name_and_path(index)\n",
                "\n",
                "    return model_name, model_path\n",
                "\n",
                "\n",
                "postfix = \"multi_chm\"\n",
                "model_name, model_path = get_model_name_and_path(postfix)\n",
                "\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    datasets[\"training\"].rgb_channels,\n",
                "    datasets[\"training\"].chm_channels,\n",
                "    device=device,\n",
                "    scale=\"n\",\n",
                "    class_names=class_names,\n",
                "    name=model_name,\n",
                ").to(device)\n",
                "\n",
                "print(f\"{datasets['training'].rgb_channels = }\")\n",
                "print(f\"{datasets['training'].chm_channels = }\")\n",
                "\n",
                "final_model = train_and_validate(\n",
                "    model=model,\n",
                "    datasets=datasets,\n",
                "    lr=lr,\n",
                "    epochs=epochs,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    accumulate=accumulate,\n",
                "    device=device,\n",
                "    save_outputs=True,\n",
                ")\n",
                "state_dict = final_model.state_dict()\n",
                "state_dict_path = os.path.join(Folders.MODELS_AMF_GD_YOLOV8.value, f\"{model_name}.pt\")\n",
                "torch.save(state_dict, state_dict_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from time import time\n",
                "# import multiprocessing as mp\n",
                "# from training import TreeDataLoader\n",
                "\n",
                "# batch_size = 8\n",
                "\n",
                "# for num_workers in range(2, mp.cpu_count() + 2, 2):\n",
                "#     train_loader = TreeDataLoader(\n",
                "#         datasets[\"training\"],\n",
                "#         batch_size=batch_size,\n",
                "#         shuffle=True,\n",
                "#         num_workers=num_workers,\n",
                "#         pin_memory=True,\n",
                "#     )\n",
                "#     start = time()\n",
                "#     for epoch in range(1, 3):\n",
                "#         for i, data in enumerate(train_loader, 0):\n",
                "#             pass\n",
                "#     end = time()\n",
                "#     print(f\"Finish with: {end - start} second, num_workers={num_workers}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f261340c63ba44c68806e284c5a78e54",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(  0%|          | 0/4 [00:00<?, ?it/s],))"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from training import test_save_output_image, initialize_dataloaders\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "index = 0\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    datasets[\"training\"].rgb_channels,\n",
                "    datasets[\"training\"].chm_channels,\n",
                "    device=device,\n",
                "    scale=\"n\",\n",
                "    class_names=class_names,\n",
                "    name=model_name,\n",
                ").to(device)\n",
                "\n",
                "state_dict = torch.load(state_dict_path)\n",
                "model.load_state_dict(state_dict)\n",
                "\n",
                "_, _, test_loader = initialize_dataloaders(\n",
                "    datasets=datasets, batch_size=batch_size, num_workers=num_workers\n",
                ")\n",
                "\n",
                "no_rgbs = [False, False, True, True]\n",
                "no_chms = [False, True, False, True]\n",
                "test_names = [\"all\", \"no_chm\", \"no_rgb\", \"no_chm_no_rgb\"]\n",
                "\n",
                "pbar = tqdm(zip(no_rgbs, no_chms, test_names), total=len(no_rgbs))\n",
                "for no_rgb, no_chm, test_name in pbar:\n",
                "    desc = f\"RGB: {'No' if no_rgb else 'Yes'}, CHM: {'No' if no_chm else 'Yes'} \"\n",
                "    pbar.set_description(desc)\n",
                "    pbar.refresh()\n",
                "    test_save_output_image(\n",
                "        model,\n",
                "        test_loader,\n",
                "        -1,\n",
                "        device,\n",
                "        no_rgb=no_rgb,\n",
                "        no_chm=no_chm,\n",
                "        save_path=os.path.join(Folders.OUTPUT_DIR.value, f\"{model_name}_{test_name}.geojson\"),\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tree-segment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
