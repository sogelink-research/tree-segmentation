{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Changes the current path to find the source files\n",
                "current_dir = os.getcwd()\n",
                "while current_dir != os.path.abspath(\"../src\"):\n",
                "    os.chdir(\"..\")\n",
                "    current_dir = os.getcwd()\n",
                "# sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils import create_all_folders, Folders\n",
                "\n",
                "create_all_folders()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from layers import AMF_GD_YOLOv8\n",
                "import torch\n",
                "from preprocessing.data import ImageData\n",
                "from training_parameters import (\n",
                "    class_names,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    transform_pixel_rgb,\n",
                "    transform_pixel_chm,\n",
                "    transform_spatial,\n",
                "    proba_drop_rgb,\n",
                "    proba_drop_chm,\n",
                "    labels_transformation_drop_chm,\n",
                "    labels_transformation_drop_rgb,\n",
                ")\n",
                "from training import (\n",
                "    train_and_validate,\n",
                "    create_and_save_splitted_datasets,\n",
                "    load_tree_datasets_from_split,\n",
                "    compute_mean_and_std,\n",
                "    compute_metrics,\n",
                "    plot_sorted_ap,\n",
                "    plot_sorted_ap_confs,\n",
                ")\n",
                "import multiprocessing as mp\n",
                "from geojson_conversions import open_geojson_feature_collection\n",
                "from preprocessing.rgb_cir import get_rgb_images_paths_from_polygon\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
                "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find and split the data\n",
                "\n",
                "annotations_file_name = \"122000_484000.geojson\"\n",
                "\n",
                "annotations_path = os.path.join(Folders.FULL_ANNOTS.value, annotations_file_name)\n",
                "annotations = open_geojson_feature_collection(annotations_path)\n",
                "full_image_path_tif = get_rgb_images_paths_from_polygon(annotations[\"bbox\"])[0]\n",
                "\n",
                "resolution = 0.08\n",
                "\n",
                "image_data = ImageData(full_image_path_tif)\n",
                "\n",
                "annotations_folder_path = os.path.join(Folders.CROPPED_ANNOTS.value, image_data.base_name)\n",
                "rgb_cir_folder_path = os.path.join(Folders.IMAGES.value, \"merged\", \"cropped\", image_data.base_name)\n",
                "chm_folder_path = os.path.join(\n",
                "    Folders.CHM.value,\n",
                "    f\"{round(resolution*100)}cm\",\n",
                "    \"filtered\",\n",
                "    \"merged\",\n",
                "    \"cropped\",\n",
                "    image_data.coord_name,\n",
                ")\n",
                "\n",
                "sets_ratios = [3, 1, 1]\n",
                "sets_names = [\"training\", \"validation\", \"test\"]\n",
                "data_split_file_path = os.path.join(Folders.OTHERS_DIR.value, \"data_split.json\")\n",
                "dismissed_classes = []\n",
                "\n",
                "create_and_save_splitted_datasets(\n",
                "    rgb_cir_folder_path,\n",
                "    chm_folder_path,\n",
                "    annotations_folder_path,\n",
                "    sets_ratios,\n",
                "    sets_names,\n",
                "    data_split_file_path,\n",
                "    random_seed=0,\n",
                ")\n",
                "\n",
                "mean_rgb, std_rgb = compute_mean_and_std(\n",
                "    rgb_cir_folder_path, per_channel=True, replace_no_data=False\n",
                ")\n",
                "no_data_new_value = -5  # TODO: Variable to add to the Dataset!\n",
                "mean_chm, std_chm = compute_mean_and_std(\n",
                "    chm_folder_path, per_channel=False, replace_no_data=True, no_data_new_value=no_data_new_value\n",
                ")\n",
                "\n",
                "datasets = load_tree_datasets_from_split(\n",
                "    data_split_file_path,\n",
                "    class_indices,\n",
                "    mean_rgb=mean_rgb,\n",
                "    std_rgb=std_rgb,\n",
                "    mean_chm=mean_chm,\n",
                "    std_chm=std_chm,\n",
                "    proba_drop_rgb=proba_drop_rgb,\n",
                "    labels_transformation_drop_rgb=labels_transformation_drop_rgb,\n",
                "    proba_drop_chm=proba_drop_chm,\n",
                "    labels_transformation_drop_chm=labels_transformation_drop_chm,\n",
                "    dismissed_classes=dismissed_classes,\n",
                "    transform_spatial_training=transform_spatial,\n",
                "    transform_pixel_rgb_training=transform_pixel_rgb,\n",
                "    transform_pixel_chm_training=transform_pixel_chm,\n",
                "    no_data_new_value=no_data_new_value,\n",
                ")\n",
                "\n",
                "# Training parameters\n",
                "\n",
                "lr = 1e-2\n",
                "epochs = 1000\n",
                "\n",
                "batch_size = 6\n",
                "num_workers = mp.cpu_count()\n",
                "accumulate = 12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "postfix = \"multi_chm\"\n",
                "model_name, model_path = AMF_GD_YOLOv8.get_new_model_name_and_path(epochs, postfix)\n",
                "\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    datasets[\"training\"].rgb_channels,\n",
                "    datasets[\"training\"].chm_channels,\n",
                "    device=device,\n",
                "    scale=\"n\",\n",
                "    class_names=class_names,\n",
                "    name=model_name,\n",
                ").to(device)\n",
                "\n",
                "print(f\"{datasets['training'].rgb_channels = }\")\n",
                "print(f\"{datasets['training'].chm_channels = }\")\n",
                "\n",
                "final_model = train_and_validate(\n",
                "    model=model,\n",
                "    datasets=datasets,\n",
                "    lr=lr,\n",
                "    epochs=epochs,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    accumulate=accumulate,\n",
                "    device=device,\n",
                "    save_outputs=False,\n",
                "    show_training_metrics=True,\n",
                ")\n",
                "\n",
                "state_dict = final_model.state_dict()\n",
                "torch.save(state_dict, model_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from time import time\n",
                "# import multiprocessing as mp\n",
                "# from training import TreeDataLoader\n",
                "\n",
                "# batch_size = 8\n",
                "\n",
                "# for num_workers in range(2, mp.cpu_count() + 2, 2):\n",
                "#     train_loader = TreeDataLoader(\n",
                "#         datasets[\"training\"],\n",
                "#         batch_size=batch_size,\n",
                "#         shuffle=True,\n",
                "#         num_workers=num_workers,\n",
                "#         pin_memory=True,\n",
                "#     )\n",
                "#     start = time()\n",
                "#     for epoch in range(1, 3):\n",
                "#         for i, data in enumerate(train_loader, 0):\n",
                "#             pass\n",
                "#     end = time()\n",
                "#     print(f\"Finish with: {end - start} second, num_workers={num_workers}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from training import test_save_output_image, initialize_dataloaders\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    datasets[\"training\"].rgb_channels,\n",
                "    datasets[\"training\"].chm_channels,\n",
                "    device=device,\n",
                "    scale=\"n\",\n",
                "    class_names=class_names,\n",
                "    name=model_name,\n",
                ").to(device)\n",
                "\n",
                "model_name, model_path = AMF_GD_YOLOv8.get_last_model_name_and_path(epochs, postfix)\n",
                "state_dict = torch.load(model_path)\n",
                "model.load_state_dict(state_dict)\n",
                "\n",
                "_, _, test_loader = initialize_dataloaders(\n",
                "    datasets=datasets, batch_size=batch_size, num_workers=num_workers\n",
                ")\n",
                "\n",
                "best_sorted_ious_list = []\n",
                "best_aps_list = []\n",
                "best_sorted_ap_list = []\n",
                "best_conf_threshold__list = []\n",
                "\n",
                "sorted_ap_lists = []\n",
                "conf_thresholds_list = []\n",
                "\n",
                "legend_list = []\n",
                "\n",
                "thresholds_low = np.power(10, np.linspace(-4, -1, 10))\n",
                "thresholds_high = np.linspace(0.1, 1.0, 19)\n",
                "conf_thresholds = np.hstack((thresholds_low, thresholds_high)).tolist()\n",
                "\n",
                "no_rgbs = [False, False, True, True]\n",
                "no_chms = [False, True, False, True]\n",
                "test_names = [\"all\", \"no_chm\", \"no_rgb\", \"no_chm_no_rgb\"]\n",
                "\n",
                "pbar = tqdm(zip(no_rgbs, no_chms, test_names), total=len(no_rgbs))\n",
                "for no_rgb, no_chm, test_name in pbar:\n",
                "    if no_rgb:\n",
                "        if no_chm:\n",
                "            legend = \"No data\"\n",
                "        else:\n",
                "            legend = \"CHM\"\n",
                "    else:\n",
                "        if no_chm:\n",
                "            legend = \"RGB\"\n",
                "        else:\n",
                "            legend = \"RGB and CHM\"\n",
                "    pbar.set_description(legend)\n",
                "    pbar.refresh()\n",
                "    test_save_output_image(\n",
                "        model,\n",
                "        test_loader,\n",
                "        -1,\n",
                "        device,\n",
                "        no_rgb=no_rgb,\n",
                "        no_chm=no_chm,\n",
                "        save_path=os.path.join(Folders.OUTPUT_DIR.value, f\"{model_name}_{test_name}.geojson\"),\n",
                "    )\n",
                "    (\n",
                "        best_sorted_ious,\n",
                "        best_aps,\n",
                "        best_sorted_ap,\n",
                "        best_conf_threshold,\n",
                "        sorted_ious_list,\n",
                "        aps_list,\n",
                "        sorted_ap_list_2,\n",
                "    ) = compute_metrics(\n",
                "        model,\n",
                "        test_loader,\n",
                "        device,\n",
                "        conf_thresholds=conf_thresholds,\n",
                "        no_rgb=no_rgb,\n",
                "        no_chm=no_chm,\n",
                "        save_path_ap_iou=os.path.join(\n",
                "            Folders.OUTPUT_DIR.value, f\"{model_name}_ap_iou_{test_name}.png\"\n",
                "        ),\n",
                "        save_path_sap_conf=os.path.join(\n",
                "            Folders.OUTPUT_DIR.value, f\"{model_name}_sap_conf_{test_name}.png\"\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    best_sorted_ious_list.append(best_sorted_ious)\n",
                "    best_aps_list.append(best_aps)\n",
                "    best_sorted_ap_list.append(best_sorted_ap)\n",
                "    best_conf_threshold__list.append(best_conf_threshold)\n",
                "\n",
                "    sorted_ap_lists.append(sorted_ap_list_2)\n",
                "    conf_thresholds_list.append(conf_thresholds)\n",
                "\n",
                "    legend_list.append(legend)\n",
                "\n",
                "plot_sorted_ap(\n",
                "    best_sorted_ious_list,\n",
                "    best_aps_list,\n",
                "    best_sorted_ap_list,\n",
                "    conf_thresholds=best_conf_threshold__list,\n",
                "    legend_list=legend_list,\n",
                "    show=True,\n",
                "    save_path=os.path.join(Folders.OUTPUT_DIR.value, f\"{model_name}_ap_iou.png\"),\n",
                ")\n",
                "\n",
                "plot_sorted_ap_confs(\n",
                "    sorted_ap_lists=sorted_ap_lists,\n",
                "    conf_thresholds_list=conf_thresholds_list,\n",
                "    legend_list=legend_list,\n",
                "    show=True,\n",
                "    save_path=os.path.join(Folders.OUTPUT_DIR.value, f\"{model_name}_sap_conf.png\"),\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tree-segment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
