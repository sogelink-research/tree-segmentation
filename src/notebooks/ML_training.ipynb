{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Changes the current path to find the source files\n",
                "current_dir = os.getcwd()\n",
                "while current_dir != os.path.abspath(\"../src\"):\n",
                "    os.chdir(\"..\")\n",
                "    current_dir = os.getcwd()\n",
                "sys.path.append(os.path.abspath(\"Efficient-Computing/Detection/Gold-YOLO\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils import create_all_folders, Folders\n",
                "\n",
                "create_all_folders()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from layers import AMF_GD_YOLOv8\n",
                "import torch\n",
                "from preprocessing.data import ImageData\n",
                "from training_parameters import (\n",
                "    class_names,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    transform_pixel_rgb,\n",
                "    transform_pixel_chm,\n",
                "    transform_spatial,\n",
                "    proba_drop_rgb,\n",
                "    proba_drop_chm,\n",
                "    labels_transformation_drop_chm,\n",
                "    labels_transformation_drop_rgb,\n",
                ")\n",
                "from training import (\n",
                "    train_and_validate,\n",
                "    create_and_save_splitted_datasets,\n",
                "    load_tree_datasets_from_split,\n",
                "    compute_mean_and_std,\n",
                ")\n",
                "import multiprocessing as mp\n",
                "from geojson_conversions import open_geojson_feature_collection\n",
                "from preprocessing.rgb_cir import download_rgb_image_from_polygon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:512\"\n",
                "print(os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find and split the data\n",
                "\n",
                "annotations_file_name = \"122000_484000.geojson\"\n",
                "\n",
                "annotations_path = os.path.join(Folders.FULL_ANNOTS.value, annotations_file_name)\n",
                "annotations = open_geojson_feature_collection(annotations_path)\n",
                "full_image_path_tif = download_rgb_image_from_polygon(annotations[\"bbox\"])[0]\n",
                "\n",
                "resolution = 0.08\n",
                "\n",
                "image_data = ImageData(full_image_path_tif)\n",
                "\n",
                "annotations_folder_path = os.path.join(Folders.CROPPED_ANNOTS.value, image_data.base_name)\n",
                "rgb_cir_folder_path = os.path.join(Folders.IMAGES.value, \"merged\", \"cropped\", image_data.base_name)\n",
                "chm_folder_path = os.path.join(\n",
                "    Folders.CHM.value,\n",
                "    f\"{round(resolution*100)}cm\",\n",
                "    \"filtered\",\n",
                "    \"merged\",\n",
                "    \"cropped\",\n",
                "    image_data.coord_name,\n",
                ")\n",
                "\n",
                "sets_ratios = [3, 1, 1]\n",
                "sets_names = [\"training\", \"validation\", \"test\"]\n",
                "data_split_file_path = \"../data/others/data_split.json\"\n",
                "dismissed_classes = []\n",
                "\n",
                "create_and_save_splitted_datasets(\n",
                "    rgb_cir_folder_path,\n",
                "    chm_folder_path,\n",
                "    annotations_folder_path,\n",
                "    sets_ratios,\n",
                "    sets_names,\n",
                "    data_split_file_path,\n",
                "    random_seed=0,\n",
                ")\n",
                "\n",
                "mean_rgb, std_rgb = compute_mean_and_std(rgb_cir_folder_path, per_channel=True)\n",
                "mean_chm, std_chm = compute_mean_and_std(chm_folder_path, per_channel=False)\n",
                "print(f\"{mean_rgb = }\")\n",
                "print(f\"{mean_chm = }\")\n",
                "\n",
                "datasets = load_tree_datasets_from_split(\n",
                "    data_split_file_path,\n",
                "    class_indices,\n",
                "    class_colors,\n",
                "    mean_rgb=mean_rgb,\n",
                "    std_rgb=std_rgb,\n",
                "    mean_chm=mean_chm,\n",
                "    std_chm=std_chm,\n",
                "    proba_drop_rgb=proba_drop_rgb,\n",
                "    labels_transformation_drop_rgb=labels_transformation_drop_rgb,\n",
                "    proba_drop_chm=proba_drop_chm,\n",
                "    labels_transformation_drop_chm=labels_transformation_drop_chm,\n",
                "    dismissed_classes=dismissed_classes,\n",
                "    transform_spatial_training=transform_spatial,\n",
                "    transform_pixel_rgb_training=transform_pixel_rgb,\n",
                "    transform_pixel_chm_training=transform_pixel_chm,\n",
                ")\n",
                "\n",
                "# Training parameters\n",
                "\n",
                "lr = 1e-2\n",
                "epochs = 400\n",
                "\n",
                "batch_size = 1\n",
                "num_workers = mp.cpu_count()\n",
                "accumulate = 12"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index = 0\n",
                "model_name = f\"trained_model_{epochs}ep_{index}_multi_chm\"\n",
                "while os.path.exists(f\"../models/amf_gd_yolov8/{model_name}.pt\"):\n",
                "    index += 1\n",
                "    model_name = f\"trained_model_{epochs}ep_{index}_multi_chm\"\n",
                "\n",
                "model = AMF_GD_YOLOv8(\n",
                "    datasets[\"training\"].rgb_channels,\n",
                "    datasets[\"training\"].chm_channels,\n",
                "    device=device,\n",
                "    scale=\"n\",\n",
                "    class_names=class_names,\n",
                "    name=model_name,\n",
                ").to(device)\n",
                "\n",
                "print(f\"{datasets['training'].rgb_channels = }\")\n",
                "print(f\"{datasets['training'].chm_channels = }\")\n",
                "\n",
                "final_model = train_and_validate(\n",
                "    model=model,\n",
                "    datasets=datasets,\n",
                "    lr=lr,\n",
                "    epochs=epochs,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    accumulate=accumulate,\n",
                "    device=device,\n",
                "    save_outputs=True,\n",
                ")\n",
                "state_dict = final_model.state_dict()\n",
                "state_dict_path = os.path.join(Folders.MODELS_AMF_GD_YOLOV8.value, f\"{model_name}.pt\")\n",
                "torch.save(state_dict, state_dict_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from time import time\n",
                "# import multiprocessing as mp\n",
                "# from training import TreeDataLoader\n",
                "\n",
                "# batch_size = 8\n",
                "\n",
                "# for num_workers in range(2, mp.cpu_count() + 2, 2):\n",
                "#     train_loader = TreeDataLoader(\n",
                "#         datasets[\"training\"],\n",
                "#         batch_size=batch_size,\n",
                "#         shuffle=True,\n",
                "#         num_workers=num_workers,\n",
                "#         pin_memory=True,\n",
                "#     )\n",
                "#     start = time()\n",
                "#     for epoch in range(1, 3):\n",
                "#         for i, data in enumerate(train_loader, 0):\n",
                "#             pass\n",
                "#     end = time()\n",
                "#     print(f\"Finish with: {end - start} second, num_workers={num_workers}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from training import test_save_output_image, initialize_dataloaders\n",
                "\n",
                "index = 0\n",
                "model_name = f\"trained_model_{epochs}ep_{index}\"\n",
                "\n",
                "model = AMF_GD_YOLOv8(3, 1, device=device, scale=\"n\", class_names=class_names, name=model_name).to(\n",
                "    device\n",
                ")\n",
                "\n",
                "state_dict = torch.load(state_dict_path)\n",
                "model.load_state_dict(state_dict)\n",
                "\n",
                "_, _, test_loader = initialize_dataloaders(\n",
                "    datasets=datasets, batch_size=batch_size, num_workers=num_workers\n",
                ")\n",
                "\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=False,\n",
                "    no_chm=False,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_all.geojson\",\n",
                ")\n",
                "\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=True,\n",
                "    no_chm=False,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_rgb.geojson\",\n",
                ")\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=False,\n",
                "    no_chm=True,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_chm.geojson\",\n",
                ")\n",
                "test_save_output_image(\n",
                "    model,\n",
                "    test_loader,\n",
                "    -1,\n",
                "    device,\n",
                "    no_rgb=True,\n",
                "    no_chm=True,\n",
                "    save_path=f\"../data/others/model_output/{model_name}_no_rgb_no_chm.geojson\",\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tree-segment",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}