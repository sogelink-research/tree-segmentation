{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from osgeo import gdal\n",
    "import pdal\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from pprint import pprint\n",
    "import laspy\n",
    "import time\n",
    "\n",
    "gdal.UseExceptions()\n",
    "\n",
    "FULL_BBOXES_FOLDER = \"../../data/annotations_full/\"\n",
    "CROPPED_BBOXES_FOLDER = \"../../data/annotations_cropped/\"\n",
    "FULL_IMAGES_FOLDER = \"../../data/images_full/\"\n",
    "CROPPED_IMAGES_FOLDER = \"../../data/images_cropped/\"\n",
    "GEOTILES_LIDAR_FOLDER = \"../../data/point_clouds_geotiles/\"\n",
    "GEOTILES_NO_OVERLAP_LIDAR_FOLDER = \"../../data/point_clouds_geotiles_no_overlap/\"\n",
    "FULL_LIDAR_FOLDER = \"../../data/point_clouds_full/\"\n",
    "CROPPED_LIDAR_FOLDER = \"../../data/point_clouds_cropped/\"\n",
    "\n",
    "folder_paths = [\n",
    "    FULL_BBOXES_FOLDER,\n",
    "    CROPPED_BBOXES_FOLDER,\n",
    "    FULL_IMAGES_FOLDER,\n",
    "    CROPPED_IMAGES_FOLDER,\n",
    "    GEOTILES_LIDAR_FOLDER,\n",
    "    GEOTILES_NO_OVERLAP_LIDAR_FOLDER,\n",
    "    FULL_LIDAR_FOLDER,\n",
    "    CROPPED_LIDAR_FOLDER,\n",
    "]\n",
    "\n",
    "# # Create the output directories if they doesn't exist\n",
    "# for folder_path in folder_paths:\n",
    "#     if not os.path.exists(folder_path):\n",
    "#         os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tile size and OVERLAP\n",
    "# TILE_SIZE = 1920  # Size of each tile\n",
    "# OVERLAP = 480  # Overlap between tiles\n",
    "TILE_SIZE = 640  # Size of each tile\n",
    "OVERLAP = 0  # Overlap between tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_execution_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        print(f\"Execution of {func.__name__}({args})...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Done in {round(execution_time, 3)} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To crop LiDAR point clouds into the size of the RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_execution_time\n",
    "def merge_crop_las(\n",
    "    input_las_list: list[str], output_las: str, x_limits: tuple, y_limits: tuple\n",
    "):\n",
    "    if x_limits[0] > x_limits[1]:\n",
    "        raise Exception(\"You should have x_limits[0] <= x_limits[1]\")\n",
    "    if y_limits[0] > y_limits[1]:\n",
    "        raise Exception(\"You should have y_limits[0] <= y_limits[1]\")\n",
    "    bounds = f\"([{x_limits[0]},{x_limits[1]}],[{y_limits[0]},{y_limits[1]}])\"\n",
    "    pipeline_list = []\n",
    "    for index, input_las in enumerate(input_las_list):\n",
    "        pipeline_list.append(\n",
    "            {\"type\": \"readers.las\", \"file_name\": input_las, \"tag\": f\"A{index}\"}\n",
    "        )\n",
    "    pipeline_list.extend(\n",
    "        [\n",
    "            {\n",
    "                \"type\": \"filters.merge\",\n",
    "                \"inputs\": [f\"A{index}\" for index in range(len(input_las_list))],\n",
    "            },\n",
    "            {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "            {\"type\": \"writers.las\", \"file_name\": output_las},\n",
    "        ]\n",
    "    )\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "@measure_execution_time\n",
    "def crop_las(input_las: str, output_las: str, x_limits: tuple, y_limits: tuple):\n",
    "    if x_limits[0] > x_limits[1]:\n",
    "        raise Exception(\"You should have x_limits[0] <= x_limits[1]\")\n",
    "    if y_limits[0] > y_limits[1]:\n",
    "        raise Exception(\"You should have y_limits[0] <= y_limits[1]\")\n",
    "    bounds = f\"([{x_limits[0]},{x_limits[1]}],[{y_limits[0]},{y_limits[1]}])\"\n",
    "    pipeline_list = [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"file_name\": input_las,\n",
    "        },\n",
    "        {\"type\": \"filters.crop\", \"bounds\": bounds},\n",
    "        {\"type\": \"writers.las\", \"file_name\": output_las},\n",
    "    ]\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "def remove_las_overlap_from_geotiles(input_las: str, output_las: str):\n",
    "    overlap = 20\n",
    "    with laspy.open(input_las, mode=\"r\") as las_file:\n",
    "        # Get the bounding box information from the header\n",
    "        min_x = las_file.header.min[0] + overlap\n",
    "        max_x = las_file.header.max[0] - overlap\n",
    "        min_y = las_file.header.min[1] + overlap\n",
    "        max_y = las_file.header.max[1] - overlap\n",
    "\n",
    "    crop_las(input_las, output_las, (min_x, max_x), (min_y, max_y))\n",
    "\n",
    "\n",
    "def remove_las_overlap_from_geotiles_all():\n",
    "    point_clouds_overlap_folder = \"../../data/point_clouds_geotiles\"\n",
    "    point_clouds_no_overlap_folder = \"../../data/point_clouds_geotiles_no_overlap\"\n",
    "    if not os.path.exists(point_clouds_no_overlap_folder):\n",
    "        os.makedirs(point_clouds_no_overlap_folder)\n",
    "    for file_name in os.listdir(point_clouds_overlap_folder):\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        overlap_file_path = os.path.join(point_clouds_overlap_folder, file_name)\n",
    "        no_overlap_file_path = os.path.join(point_clouds_no_overlap_folder, file_name)\n",
    "        if (os.path.isfile(overlap_file_path)) and (\n",
    "            not os.path.exists(no_overlap_file_path)\n",
    "        ):\n",
    "            remove_las_overlap_from_geotiles(overlap_file_path, no_overlap_file_path)\n",
    "\n",
    "\n",
    "@measure_execution_time\n",
    "def filter_classification_las(input_las: str, output_las: str):\n",
    "    pipeline_list = [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"file_name\": input_las,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.range\",\n",
    "            \"limits\": \"Classification[1:5]\",  # Keep only unclassified, ground and vegetation\n",
    "        },\n",
    "        {\"type\": \"writers.las\", \"file_name\": output_las},\n",
    "    ]\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "# # Open the GeoTIFF file\n",
    "# tiff_image_test = \"../../data/images_full/2023_122000_484000_RGB_hrl.tif\"\n",
    "# ds = gdal.Open(tiff_image_test)\n",
    "# # Get the geotransform parameters\n",
    "# gt = ds.GetGeoTransform()\n",
    "# # Calculate the image coordinates\n",
    "# width = ds.RasterXSize\n",
    "# height = ds.RasterYSize\n",
    "# # Calculate the coordinates of the four corners\n",
    "# x1 = int(gt[0])\n",
    "# y1 = int(gt[3])\n",
    "# x2 = int(gt[0] + (gt[1] * width))\n",
    "# y2 = int(gt[3] + (gt[5] * height))\n",
    "# # Close the dataset\n",
    "# ds = None\n",
    "# # Crop the point cloud\n",
    "# geotiles_point_clouds_path = [\"../../data/point_clouds_geotiles/25GN1_13.LAZ\", \"../../data/point_clouds_geotiles/25GN1_18.LAZ\"]\n",
    "# geotiles_no_overlap_point_clouds_path = [\"../../data/point_clouds_geotiles_no_overlap/25GN1_13.LAZ\", \"../../data/point_clouds_geotiles_no_overlap/25GN1_18.LAZ\"]\n",
    "# for overlap_path, no_overlap_path in zip(geotiles_point_clouds_path, geotiles_no_overlap_point_clouds_path):\n",
    "#     remove_las_overlap_from_geotiles(overlap_path, no_overlap_path)\n",
    "# full_point_cloud_path = f\"../../data/point_clouds_full/{int(x1)}_{int(y1)}.laz\"\n",
    "# crop_las(geotiles_no_overlap_point_clouds_path, full_point_cloud_path, (x1, x2), (y2, y1))\n",
    "# full_point_filtered_cloud_path = f\"../../data/point_clouds_full/{int(x1)}_{int(y1)}_filtered.laz\"\n",
    "# filter_classification_las(full_point_cloud_path, full_point_filtered_cloud_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_path = FULL_BBOXES_FOLDER + \"3\"\n",
    "\n",
    "with open(bboxes_path, \"r\") as file:\n",
    "    # Load the annotation data\n",
    "    bboxes_json = json.load(file)\n",
    "\n",
    "    # Get the path to the full image\n",
    "    full_image_path = bboxes_json[\"task\"][\"data\"][\"image\"].replace(\n",
    "        \"/data/local-files/?d=\", \"/\"\n",
    "    )\n",
    "    full_image_path_tif = full_image_path.replace(\".png\", \".tif\")\n",
    "    full_image_path_tif = \"../../data/images_full/2023_122000_484000_RGB_hrl.tif\"\n",
    "\n",
    "    # Create the paths\n",
    "    output_image_prefix = os.path.splitext(os.path.basename(full_image_path_tif))[0]\n",
    "    annotation_output_directory = os.path.join(\n",
    "        CROPPED_BBOXES_FOLDER, output_image_prefix\n",
    "    )\n",
    "    if not os.path.exists(annotation_output_directory):\n",
    "        os.makedirs(annotation_output_directory)\n",
    "\n",
    "    # Get the dimensions of the full image\n",
    "    full_image = Image.open(full_image_path_tif)\n",
    "    full_image_width, full_image_height = full_image.size\n",
    "    full_image_width_factor, full_image_height_factor = (\n",
    "        full_image_width / 100.0,\n",
    "        full_image_height / 100.0,\n",
    "    )\n",
    "\n",
    "    # Calculate the number of rows and columns needed\n",
    "    num_cols = int(np.ceil((full_image_width - OVERLAP) / (TILE_SIZE - OVERLAP)))\n",
    "    num_rows = int(np.ceil((full_image_height - OVERLAP) / (TILE_SIZE - OVERLAP)))\n",
    "\n",
    "    # Get the limits of all the cropped images\n",
    "    cropping_limits_x = np.array(\n",
    "        [\n",
    "            [i * (TILE_SIZE - OVERLAP), (i + 1) * (TILE_SIZE - OVERLAP) + OVERLAP]\n",
    "            for i in range(num_cols)\n",
    "        ]\n",
    "    )\n",
    "    cropping_limits_y = np.array(\n",
    "        [\n",
    "            [j * (TILE_SIZE - OVERLAP), (j + 1) * (TILE_SIZE - OVERLAP) + OVERLAP]\n",
    "            for j in range(num_rows)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    bboxes_repartition = [[[] for _ in range(num_rows)] for _ in range(num_cols)]\n",
    "    for index, bbox_info in enumerate(bboxes_json[\"result\"]):\n",
    "        bbox = bbox_info[\"value\"]\n",
    "        min_x = int(np.round(bbox[\"x\"] * full_image_width_factor))\n",
    "        min_y = int(np.round(bbox[\"y\"] * full_image_height_factor))\n",
    "        max_x = int(np.round((bbox[\"x\"] + bbox[\"width\"]) * full_image_width_factor))\n",
    "        max_y = int(np.round((bbox[\"y\"] + bbox[\"height\"]) * full_image_height_factor))\n",
    "        # Find the indices of the cropped images in which the bounding box fits\n",
    "        i_x_0 = min_x // (TILE_SIZE - OVERLAP)\n",
    "        i_y_0 = min_y // (TILE_SIZE - OVERLAP)\n",
    "\n",
    "        found_image = False  # To check if the bounding box fits entirely in at least one cropped image\n",
    "\n",
    "        # Check the 4 possibilities\n",
    "        if max_x < (i_x_0 + 1) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "            # First possible image (bottom right)\n",
    "            if max_y < (i_y_0 + 1) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "                bboxes_repartition[i_x_0][i_y_0].append(index)\n",
    "                found_image = True\n",
    "            # Second possible image (top right)\n",
    "            if max_y < (i_y_0) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "                bboxes_repartition[i_x_0][i_y_0 - 1].append(index)\n",
    "                found_image = True\n",
    "        if max_x < (i_x_0) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "            # Third possible image (bottom left)\n",
    "            if max_y < (i_y_0 + 1) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "                bboxes_repartition[i_x_0 - 1][i_y_0].append(index)\n",
    "                found_image = True\n",
    "            # Fourth possible image (top left)\n",
    "            if max_y < (i_y_0) * (TILE_SIZE - OVERLAP) + OVERLAP:\n",
    "                bboxes_repartition[i_x_0 - 1][i_y_0 - 1].append(index)\n",
    "                found_image = True\n",
    "\n",
    "        if not found_image:\n",
    "            raise Exception(\n",
    "                f\"The bounding box at index {index} doesn't fit entirely in any image.\"\n",
    "            )\n",
    "\n",
    "    # Create and store the cropped annotation files\n",
    "    for row in tqdm(range(num_rows)):\n",
    "        for col in tqdm(range(num_cols), leave=False):\n",
    "            bboxes_dict = {\n",
    "                \"full_image\": {\n",
    "                    \"path\": full_image_path_tif,\n",
    "                    \"coordinates_of_cropped_image\": {\n",
    "                        \"x\": col * (TILE_SIZE - OVERLAP),\n",
    "                        \"y\": row * (TILE_SIZE - OVERLAP),\n",
    "                        \"width\": TILE_SIZE,\n",
    "                        \"height\": TILE_SIZE,\n",
    "                    },\n",
    "                    \"overlap\": OVERLAP,\n",
    "                },\n",
    "                \"col\": col,\n",
    "                \"row\": row,\n",
    "                \"width\": TILE_SIZE,\n",
    "                \"height\": TILE_SIZE,\n",
    "                \"bounding_boxes\": [\n",
    "                    {\n",
    "                        \"id\": bboxes_json[\"result\"][i][\"id\"],\n",
    "                        \"index\": i,\n",
    "                        \"x\": bboxes_json[\"result\"][i][\"value\"][\"x\"]\n",
    "                        * full_image_width_factor\n",
    "                        - col * (TILE_SIZE - OVERLAP),\n",
    "                        \"y\": bboxes_json[\"result\"][i][\"value\"][\"y\"]\n",
    "                        * full_image_width_factor\n",
    "                        - row * (TILE_SIZE - OVERLAP),\n",
    "                        \"width\": bboxes_json[\"result\"][i][\"value\"][\"width\"]\n",
    "                        * full_image_width_factor,\n",
    "                        \"height\": bboxes_json[\"result\"][i][\"value\"][\"height\"]\n",
    "                        * full_image_width_factor,\n",
    "                        \"label\": bboxes_json[\"result\"][i][\"value\"][\"rectanglelabels\"][\n",
    "                            0\n",
    "                        ],\n",
    "                    }\n",
    "                    for i in bboxes_repartition[col][row]\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            annotation_output_file_name = f\"{output_image_prefix}_{row}_{col}.json\"\n",
    "            output_path = os.path.join(\n",
    "                annotation_output_directory, annotation_output_file_name\n",
    "            )\n",
    "            with open(output_path, \"w\") as outfile:\n",
    "                json.dump(bboxes_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this can also be done using gdal_retile.py\n",
    "\n",
    "# Define output paths\n",
    "image_output_directory = os.path.join(CROPPED_IMAGES_FOLDER, output_image_prefix)\n",
    "if not os.path.exists(image_output_directory):\n",
    "    os.makedirs(image_output_directory)\n",
    "\n",
    "# Iterate over rows and columns to create tiles\n",
    "for row in tqdm(range(num_rows)):\n",
    "    for col in tqdm(range(num_cols), leave=False):\n",
    "        # Calculate the pixel offsets for the tile\n",
    "        x_offset = col * (TILE_SIZE - OVERLAP)\n",
    "        y_offset = row * (TILE_SIZE - OVERLAP)\n",
    "\n",
    "        # Create output file_name\n",
    "        output_file_name = f\"{output_image_prefix}_{row}_{col}.tif\"\n",
    "        output_path = os.path.join(image_output_directory, output_file_name)\n",
    "\n",
    "        # Define the subset area to read from the input image\n",
    "        window = (x_offset, y_offset, TILE_SIZE, TILE_SIZE)\n",
    "\n",
    "        gdal.Translate(output_path, full_image_path_tif, srcWin=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_las_overlap_from_geotiles_all()\n",
    "\n",
    "\n",
    "def get_coordinates_from_image_file_name(file_name: str):\n",
    "    return (int(file_name[5:11]), int(file_name[12:18]))\n",
    "\n",
    "\n",
    "output_coordinates = get_coordinates_from_image_file_name(output_image_prefix)\n",
    "output_coordinates_prefix = f\"{output_coordinates[0]}_{output_coordinates[1]}\"\n",
    "\n",
    "# Define output paths\n",
    "point_cloud_output_directory = os.path.join(\n",
    "    CROPPED_LIDAR_FOLDER, output_coordinates_prefix\n",
    ")\n",
    "if not os.path.exists(point_cloud_output_directory):\n",
    "    os.makedirs(point_cloud_output_directory)\n",
    "\n",
    "full_point_cloud_path = (\n",
    "    f\"../../data/point_clouds_full/{output_coordinates_prefix}_filtered.laz\"\n",
    ")\n",
    "\n",
    "# Iterate over rows and columns to create tiles\n",
    "for row in tqdm(range(num_rows)):\n",
    "    for col in tqdm(range(num_cols), leave=False):\n",
    "        # Create output file_name\n",
    "        output_point_cloud_file_name = f\"{output_coordinates_prefix}_{row}_{col}.laz\"\n",
    "        output_point_cloud_path = os.path.join(\n",
    "            point_cloud_output_directory, output_point_cloud_file_name\n",
    "        )\n",
    "        output_point_cloud_filtered_file_name = (\n",
    "            f\"{output_coordinates_prefix}_{row}_{col}_filtered.laz\"\n",
    "        )\n",
    "        output_point_cloud_filtered_path = os.path.join(\n",
    "            point_cloud_output_directory, output_point_cloud_filtered_file_name\n",
    "        )\n",
    "\n",
    "        if os.path.exists(output_point_cloud_path) and os.path.exists(\n",
    "            output_point_cloud_filtered_path\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Get the corresponding image\n",
    "        corresponding_image_file_name = f\"{output_image_prefix}_{row}_{col}.tif\"\n",
    "        corresponding_image_path = os.path.join(\n",
    "            image_output_directory, corresponding_image_file_name\n",
    "        )\n",
    "        print(f\"{corresponding_image_path = }\")\n",
    "\n",
    "        ### Get the coordinates\n",
    "        ds = gdal.Open(corresponding_image_path)\n",
    "        # Get the geotransform parameters\n",
    "        gt = ds.GetGeoTransform()\n",
    "        # Calculate the image coordinates\n",
    "        width = ds.RasterXSize\n",
    "        height = ds.RasterYSize\n",
    "        # Calculate the coordinates of the four corners\n",
    "        x1 = round(gt[0], 3)\n",
    "        y1 = round(gt[3], 3)\n",
    "        x2 = round(gt[0] + (gt[1] * width), 3)\n",
    "        y2 = round(gt[3] + (gt[5] * height), 3)\n",
    "        # Close the dataset\n",
    "        ds = None\n",
    "\n",
    "        print(f\"{(x1, x2, y1, y2) = }\")\n",
    "\n",
    "        crop_las(full_point_cloud_path, output_point_cloud_path, (x1, x2), (y2, y1))\n",
    "        filter_classification_las(\n",
    "            output_point_cloud_path, output_point_cloud_filtered_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_boxes(image_path: str, boxes: list | None = None) -> None:\n",
    "    reduction_ratio = 3\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    image_smaller = Image.open(image_path)\n",
    "    image_smaller.thumbnail(\n",
    "        (TILE_SIZE // reduction_ratio, TILE_SIZE // reduction_ratio)\n",
    "    )\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(20, 40))\n",
    "\n",
    "    # Display image\n",
    "    axs[0].imshow(image)\n",
    "    axs[1].imshow(image_smaller)\n",
    "\n",
    "    # Annotation colors\n",
    "    colors = {\n",
    "        \"Tree\": \"#9effb1\",\n",
    "        \"Tree_unsure\": \"#ffd79e\",\n",
    "        \"Tree_disappeared\": \"#9eaeff\",\n",
    "        \"Tree_replaced\": \"#ff5a52\",\n",
    "        \"Tree_new\": \"#fb6ae1\",\n",
    "    }\n",
    "\n",
    "    # Add bounding boxes if provided\n",
    "    if boxes:\n",
    "        for box in boxes:\n",
    "            # Extract box coordinates\n",
    "            x, y, width, height, label = box\n",
    "            # Create a Rectangle patch\n",
    "            rect = Rectangle(\n",
    "                (x, y),\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=1,\n",
    "                edgecolor=colors[label],\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            # Add the patch to the Axes\n",
    "            axs[0].add_patch(rect)\n",
    "            # Create a Rectangle patch\n",
    "            rect = Rectangle(\n",
    "                (x // reduction_ratio, y // reduction_ratio),\n",
    "                width // reduction_ratio,\n",
    "                height // reduction_ratio,\n",
    "                linewidth=1,\n",
    "                edgecolor=colors[label],\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            # Add the patch to the Axes\n",
    "            axs[1].add_patch(rect)\n",
    "\n",
    "    # Add each rectangle to the legend individually\n",
    "    for label, color in colors.items():\n",
    "        axs[0].add_patch(Rectangle((0, 0), 0, 0, color=color, label=label))\n",
    "        axs[1].add_patch(Rectangle((0, 0), 0, 0, color=color, label=label))\n",
    "\n",
    "    axs[0].set_axis_off()\n",
    "    axs[0].legend()\n",
    "    axs[1].set_axis_off()\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_bounding_boxes(bboxes_path: str) -> list:\n",
    "    with open(bboxes_path, \"r\") as file:\n",
    "        # Load the annotation data\n",
    "        bboxes_json = json.load(file)\n",
    "\n",
    "        # Get every bounding box\n",
    "        bboxes = []\n",
    "        for bbox in bboxes_json[\"bounding_boxes\"]:\n",
    "            bboxes.append(\n",
    "                (bbox[\"x\"], bbox[\"y\"], bbox[\"width\"], bbox[\"height\"], bbox[\"label\"])\n",
    "            )\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "image_path = \"../../data/images_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_3.tif\"\n",
    "bboxes_path = \"../../data/annotations_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_1_3.json\"\n",
    "\n",
    "# bboxes = get_bounding_boxes(bboxes_path)\n",
    "# display_image_with_boxes(image_path, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate points removal (post-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_points(input_las_file, output_las_file):\n",
    "    # Open the input LAS file\n",
    "    in_las = laspy.read(input_las_file)\n",
    "    # Convert coordinates and all dimensions to a NumPy array\n",
    "    points = np.column_stack(\n",
    "        [getattr(in_las, dim) for dim in in_las.point_format.dimension_names]\n",
    "    )\n",
    "    # Find unique points\n",
    "    unique_points, idx = np.unique(points, axis=0, return_index=True)\n",
    "    # Extract unique coordinates\n",
    "    num_points = len(unique_points)\n",
    "    # Create a new LAS file\n",
    "    out_header = in_las.header\n",
    "    out_header.point_count = num_points\n",
    "    out_las = laspy.LasData(out_header)\n",
    "    # Set all dimensions in the output LAS file\n",
    "    for dim_idx, dim_name in enumerate(in_las.point_format.dimension_names):\n",
    "        dim_values = unique_points[:, dim_idx]\n",
    "        # If the dimension is not used, don't set it\n",
    "        if np.all(dim_values == dim_values[0]):\n",
    "            print(dim_name)\n",
    "            continue\n",
    "        setattr(out_las, dim_name, dim_values)\n",
    "    out_las.write(output_las_file)\n",
    "\n",
    "\n",
    "def las_to_laz(input_las_path: str):\n",
    "    input_las_path_no_extension, initial_extension = os.path.splitext(input_las_path)\n",
    "    if initial_extension not in [\".las\", \".LAS\"]:\n",
    "        raise Exception(\"The input must be a LAS file.\")\n",
    "    output_laz_path = input_las_path_no_extension + \".laz\"\n",
    "    pipeline_list = [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"file_name\": input_las_path,\n",
    "        },\n",
    "        {\"type\": \"writers.las\", \"file_name\": output_laz_path},\n",
    "    ]\n",
    "    pprint(json.dumps(pipeline_list))\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# input_las_file = \"../../data/point_clouds_full/122000_484000_with_duplicates_2.las\"\n",
    "# output_las_file = \"../../data/point_clouds_full/122000_484000_2.laz\"\n",
    "# output_las_file_2 = \"../../data/point_clouds_full/122000_484000_3_2.las\"\n",
    "# las_to_laz(input_las_file)\n",
    "# remove_duplicate_points(input_las_file, output_las_file)\n",
    "# remove_duplicate_points(input_las_file, output_las_file_2)\n",
    "# las_to_laz(output_las_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates verifications for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the coordinates\n",
    "ds = gdal.Open(\n",
    "    \"../../data/images_cropped/2023_122000_484000_RGB_hrl/2023_122000_484000_RGB_hrl_0_0.tif\"\n",
    ")\n",
    "# Get the geotransform parameters\n",
    "gt = ds.GetGeoTransform()\n",
    "# Calculate the image coordinates\n",
    "width = ds.RasterXSize\n",
    "height = ds.RasterYSize\n",
    "# Calculate the coordinates of the four corners\n",
    "x1 = round(gt[0], 3)\n",
    "y1 = round(gt[3], 3)\n",
    "x2 = round(gt[0] + (gt[1] * width), 3)\n",
    "y2 = round(gt[3] + (gt[5] * height), 3)\n",
    "# Close the dataset\n",
    "ds = None\n",
    "\n",
    "print(\"Minimum X:\", x1)\n",
    "print(\"Maximum X:\", x2)\n",
    "print(\"Minimum Y:\", y1)\n",
    "print(\"Maximum Y:\", y2)\n",
    "print(f\"{width = }\")\n",
    "print(f\"{height = }\")\n",
    "\n",
    "with laspy.open(\n",
    "    \"../../data/point_clouds_cropped/122000_484000/122000_484000_0_0_filtered.laz\",\n",
    "    mode=\"r\",\n",
    ") as las_file:\n",
    "    # Get the bounding box information from the header\n",
    "    min_x = las_file.header.min[0]\n",
    "    max_x = las_file.header.max[0]\n",
    "    min_y = las_file.header.min[1]\n",
    "    max_y = las_file.header.max[1]\n",
    "    min_z = las_file.header.min[2]\n",
    "    max_z = las_file.header.max[2]\n",
    "\n",
    "# Print out the bounds\n",
    "print(\"Minimum X:\", min_x)\n",
    "print(\"Maximum X:\", max_x)\n",
    "print(\"Minimum Y:\", min_y)\n",
    "print(\"Maximum Y:\", max_y)\n",
    "print(\"Width:\", max_x - min_x)\n",
    "print(\"Height:\", max_y - min_y)\n",
    "\n",
    "# with laspy.open(f\"../../data/point_clouds_test/122000_484000_filtered_1.laz\", mode=\"r\") as las_file:\n",
    "#     # Get the bounding box information from the header\n",
    "#     min_x = las_file.header.min[0]\n",
    "#     max_x = las_file.header.max[0]\n",
    "#     min_y = las_file.header.min[1]\n",
    "#     max_y = las_file.header.max[1]\n",
    "#     min_z = las_file.header.min[2]\n",
    "#     max_z = las_file.header.max[2]\n",
    "\n",
    "# # Print out the bounds\n",
    "# print(\"Minimum X:\", min_x)\n",
    "# print(\"Maximum X:\", max_x)\n",
    "# print(\"Minimum Y:\", min_y)\n",
    "# print(\"Maximum Y:\", max_y)\n",
    "# print(\"Width:\", max_x - min_x)\n",
    "# print(\"Height:\", max_y - min_y)\n",
    "\n",
    "print(1920 / 153.6)\n",
    "print(1920 * 153.6 / 1920)\n",
    "print(480 * 153.6 / 1920)\n",
    "print((1920 - 480) * 153.6 / 1920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop point clouds in one PDAL pipeline (not working yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_point_cloud(input_file, output_folder):\n",
    "    # Construct the PDAL pipeline\n",
    "    pipeline_list = [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"file_name\": input_file,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.splitter\",\n",
    "            \"length\": \"50\",\n",
    "            \"buffer\": \"20\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"writers.las\",\n",
    "            \"file_name\": os.path.join(\n",
    "                output_folder,\n",
    "                f\"{os.path.basename(os.path.splitext(input_file)[0])}_#.laz\",\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "    pipeline.execute()\n",
    "\n",
    "\n",
    "# Specify the input LAS/LAZ file\n",
    "input_file = \"../../data/point_clouds_full/122000_484000_filtered.laz\"\n",
    "\n",
    "# Specify the output folder\n",
    "point_cloud_test_output_folder = \"../../data/point_clouds_test\"\n",
    "if not os.path.exists(point_cloud_test_output_folder):\n",
    "    os.makedirs(point_cloud_test_output_folder)\n",
    "\n",
    "# Crop the point cloud into tiles\n",
    "crop_point_cloud(input_file, point_cloud_test_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the point clouds corresponding to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "from shapely.geometry import box\n",
    "from shapely.wkt import dumps\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Open the file in binary write mode and write the content of the response\n",
    "        print(f\"Downloading {url}...\", end=\" \", flush=True)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Saved as '{save_path}'\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Failed to download file from '{url}'. Status code: {response.status_code}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Path to your TIF image\n",
    "tif_file = full_image_path_tif\n",
    "\n",
    "# Path to your Shapefile\n",
    "shapefile = (\n",
    "    \"../../data/point_clouds_geotiles/TOP-AHN_subunit_compat/TOP-AHN_subunit_compat.shp\"\n",
    ")\n",
    "\n",
    "# Open the TIF image\n",
    "ds = gdal.Open(tif_file)\n",
    "\n",
    "# Get the geotransform and projection\n",
    "gt = ds.GetGeoTransform()\n",
    "proj = ds.GetProjection()\n",
    "\n",
    "# Get the extent of the TIF image\n",
    "min_x = gt[0]\n",
    "max_y = gt[3]\n",
    "max_x = min_x + gt[1] * ds.RasterXSize\n",
    "min_y = max_y + gt[5] * ds.RasterYSize\n",
    "\n",
    "# Create a box geometry representing the extent of the TIF image\n",
    "overlap = 20\n",
    "bbox = box(min_x + overlap, min_y + overlap, max_x - overlap, max_y - overlap)\n",
    "\n",
    "# Convert the Shapely geometry to an ogr.Geometry object\n",
    "bbox_ogr = ogr.CreateGeometryFromWkt(dumps(bbox))\n",
    "\n",
    "# Open the Shapefile\n",
    "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "shp_ds = driver.Open(shapefile, 0)\n",
    "layer = shp_ds.GetLayer()\n",
    "\n",
    "# Get the intersection between TIF image and Shapefile\n",
    "intersection_file_names = []\n",
    "for feature in layer:\n",
    "    geom = feature.GetGeometryRef()\n",
    "    if geom.Intersects(bbox_ogr):\n",
    "        intersection_file_names.append(\n",
    "            feature.GetField(\"AHN\")\n",
    "        )  # Replace \"file_name_column\" with the actual column name\n",
    "\n",
    "# Close the Shapefile\n",
    "shp_ds = None\n",
    "\n",
    "\n",
    "intersection_file_paths = []\n",
    "for file_name in intersection_file_names:\n",
    "    url = f\"https://geotiles.citg.tudelft.nl/AHN4_T/{file_name}.LAZ\"\n",
    "    # Create the paths\n",
    "    geotiles_with_overlap_path = os.path.join(GEOTILES_LIDAR_FOLDER, f\"{file_name}.LAZ\")\n",
    "    geotiles_without_overlap_path = os.path.join(\n",
    "        GEOTILES_NO_OVERLAP_LIDAR_FOLDER, f\"{file_name}.LAZ\"\n",
    "    )\n",
    "    intersection_file_paths.append(geotiles_without_overlap_path)\n",
    "    # Download the point clouds\n",
    "    if not os.path.exists(geotiles_with_overlap_path):\n",
    "        download_file(url, geotiles_with_overlap_path)\n",
    "    # Remove the overlap from the point clouds\n",
    "    if not os.path.exists(geotiles_without_overlap_path):\n",
    "        remove_las_overlap_from_geotiles(\n",
    "            geotiles_with_overlap_path, geotiles_without_overlap_path\n",
    "        )\n",
    "\n",
    "# Crop the point clouds into the area of the full image\n",
    "full_point_cloud_path = f\"../../data/point_clouds_full/{int(min_x)}_{int(max_y)}.laz\"\n",
    "if not os.path.exists(full_point_cloud_path):\n",
    "    merge_crop_las(\n",
    "        intersection_file_paths, full_point_cloud_path, (min_x, max_x), (min_y, max_y)\n",
    "    )\n",
    "\n",
    "# Filter the full point cloud to remove buildings\n",
    "full_point_filtered_cloud_path = (\n",
    "    f\"../../data/point_clouds_full/{int(min_x)}_{int(max_y)}_filtered.laz\"\n",
    ")\n",
    "if not os.path.exists(full_point_filtered_cloud_path):\n",
    "    filter_classification_las(full_point_cloud_path, full_point_filtered_cloud_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform QGIS bounding boxes to Label Studio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes_qgis_to_label_studio(\n",
    "    input_folder: str, output_json_path: str, image_path: str, task_id: int\n",
    ") -> None:\n",
    "    bboxes_dict = {\n",
    "        \"id\": task_id,\n",
    "        \"annotations\": [{\"result\": []}],\n",
    "        \"data\": {\"image\": f\"/data/local-files/?d={image_path}\"},\n",
    "    }\n",
    "\n",
    "    ds = gdal.Open(image_path)\n",
    "    gt = ds.GetGeoTransform()\n",
    "    width = ds.RasterXSize\n",
    "    height = ds.RasterYSize\n",
    "    xmin = round(gt[0], 3)\n",
    "    ymax = round(gt[3], 3)\n",
    "    xmax = round(gt[0] + (gt[1] * width), 3)\n",
    "    ymin = round(gt[3] + (gt[5] * height), 3)\n",
    "    width_factor = 100 / (xmax - xmin)\n",
    "    height_factor = 100 / (ymax - ymin)\n",
    "    ds = None\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        geojson_file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.splitext(geojson_file_path)[1] == \".geojson\":\n",
    "            with open(geojson_file_path, \"r\") as file:\n",
    "                bbox_json = json.load(file)\n",
    "                coordinates = bbox_json[\"features\"][0][\"geometry\"][\"coordinates\"][0]\n",
    "                x0 = coordinates[0][0]\n",
    "                y0 = coordinates[0][1]\n",
    "                x1 = coordinates[2][0]\n",
    "                y1 = coordinates[2][1]\n",
    "                bboxes_dict[\"annotations\"][0][\"result\"].append(\n",
    "                    {\n",
    "                        \"original_width\": 12500,\n",
    "                        \"original_height\": 12500,\n",
    "                        \"value\": {\n",
    "                            \"x\": (x0 - xmin) * width_factor,\n",
    "                            \"y\": (ymax - y1 + 1) * height_factor,\n",
    "                            \"width\": (x1 - x0) * width_factor,\n",
    "                            \"height\": (y1 - y0) * height_factor,\n",
    "                            \"rotation\": 0,\n",
    "                            \"rectanglelabels\": [\"Tree\"],\n",
    "                        },\n",
    "                        \"from_name\": \"label\",\n",
    "                        \"to_name\": \"image\",\n",
    "                        \"type\": \"rectanglelabels\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    with open(output_json_path, \"w\") as outfile:\n",
    "        json.dump([bboxes_dict], outfile)\n",
    "\n",
    "\n",
    "input_folder = \"../../data/QGIS_bounding_boxes\"\n",
    "output_json = \"../../data/images_full/label_studio_pre_annotations.json\"\n",
    "image_path = \"/home/alexandre/Documents/tree-segmentation/data/images/full/2023_122000_484000_RGB_hrl.png\"\n",
    "task_id = 1\n",
    "\n",
    "bboxes_qgis_to_label_studio(input_folder, output_json, image_path, task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes_label_studio_to_label_studio(\n",
    "    input_json_path: str, output_json_path: str, image_path: str, task_id: int\n",
    "):\n",
    "    bboxes_dict = {\n",
    "        # \"id\": task_id,\n",
    "        \"data\": {\"image\": f\"/data/local-files/?d={image_path}\"},\n",
    "        \"annotations\": [{\"result\": []}],\n",
    "    }\n",
    "\n",
    "    with open(input_json_path, \"r\") as file:\n",
    "        bbox_json = json.load(file)\n",
    "        bboxes_dict[\"annotations\"][0][\"result\"] = bbox_json[\"result\"]\n",
    "\n",
    "    with open(output_json_path, \"w\") as outfile:\n",
    "        json.dump(bboxes_dict, outfile)\n",
    "\n",
    "\n",
    "input_file = \"../../data/annotations/full/6\"\n",
    "output_json = \"../../data/label_studio/medium_trees.json\"\n",
    "image_path = \"home/alexandre/Documents/tree-segmentation/data/images/full/2023_122000_484000_RGB_hrl.png\"\n",
    "task_id = 1\n",
    "\n",
    "bboxes_label_studio_to_label_studio(input_file, output_json, image_path, task_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
